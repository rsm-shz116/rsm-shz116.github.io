[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Multinomial Logit Model\n\n\n\n\n\n\nShuyin Zheng\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\nShuyin Zheng\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nShuyin Zheng\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nShuyin Zheng\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Project 2/hw2_questions.html",
    "href": "blog/Project 2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\ndf &lt;- read.csv(\"blueprinty.csv\")\nhead(df)\n\n  patents    region  age iscustomer\n1       0   Midwest 32.5          0\n2       3 Southwest 37.5          0\n3       4 Northwest 27.0          1\n4       3 Northeast 24.5          0\n5       3 Southwest 37.0          0\n6       6 Northeast 29.5          1\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(position = \"dodge\",bins = 30) +\n  labs(title = \"Histogram of Patents by Customer Status\",\n       x = \"Number of Patents\", fill = \"Customer\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\naggregate(patents ~ iscustomer, data = df, mean)\n\n  iscustomer  patents\n1          0 3.473013\n2          1 4.133056\n\n\n\nThe histogram shows a right-skewed distribution of patents for both Blueprinty customers and non-customers, with the majority of firms holding between 0 and 6 patents. However, the distribution for customers appears slightly shifted to the right, suggesting that they tend to have more patents on average. Non-customers have an average of 3.47 patents.Customers have a higher average of 4.13 patents. This difference suggests that Blueprinty customers tend to have more patents than non-customers. However, because customers are not randomly assigned, this observed difference may be influenced by systematic differences in firm characteristics—such as age or region. A more rigorous model (e.g., regression) is necessary to isolate the potential causal effect of using Blueprinty’s software.\n\nTo better understand potential selection bias, we now examine how firm characteristics—specifically age and regional location—differ by customer status. This helps us assess whether Blueprinty customers are systematically different from non-customers.\n\nggplot(df, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +\n  geom_boxplot() +\n  labs(title = \"Firm Age by Customer Status\",\n       x = \"Customer (0 = No, 1 = Yes)\", y = \"Age\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\naggregate(age ~ iscustomer, data = df, mean)\n\n  iscustomer      age\n1          0 26.10157\n2          1 26.90021\n\n\n\nAge The boxplot shows that Blueprinty customers and non-customers have similar age distributions, though customers tend to be slightly older. The average age of customers is 26.90 years, while non-customers average 26.10 years. This suggests that customers are marginally more established on average.\n\n\nggplot(df, aes(x = region, fill = factor(iscustomer))) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Region Distribution by Customer Status\",\n       x = \"Region\", y = \"Proportion\", fill = \"Customer\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nprop.table(table(df$region, df$iscustomer), 1)\n\n           \n                    0         1\n  Midwest   0.8348214 0.1651786\n  Northeast 0.4542429 0.5457571\n  Northwest 0.8449198 0.1550802\n  South     0.8167539 0.1832461\n  Southwest 0.8249158 0.1750842\n\n\n\nRegion The stacked bar plot and proportion table reveal notable differences in regional composition: In the Northeast, over 54% of firms are Blueprinty customers — the only region where customers outnumber non-customers. In all other regions (Midwest, Northwest, South, Southwest), customers make up roughly 15%–18% of firms.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nFormula for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\): \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n\\[\nP(Y_i = y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\n\\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + y_i \\log \\lambda - \\log y_i! \\right)\n\\] To estimate the Poisson model, we begin by expressing its likelihood mathematically and implementing the corresponding log-likelihood function in R.\n\npoisson_loglikelihood &lt;- function(lambda, Y){\n  if (lambda &lt;= 0) {\n    return(-Inf)  \n  }\n  n &lt;- length(Y)\n  log_likelihood &lt;- -n * lambda + sum(Y) * log(lambda) - sum(lgamma(Y + 1))\n  return(log_likelihood)\n}\n\nWith the log-likelihood function defined, we now visualize how it behaves across a range of λ values using the observed patent counts as input.\n\nY &lt;- df$patents\n\n\nlambda_values &lt;- seq(0.1, 10, length.out = 200)\nlog_likelihoods &lt;- sapply(lambda_values, poisson_loglikelihood, Y = Y)\n\nplot(lambda_values, log_likelihoods, type = \"l\",\n     xlab = \"Lambda\", ylab = \"Log-Likelihood\",\n     main = \"Poisson Log-Likelihood as a Function of Lambda\")\n\n\n\n\n\n\n\n\nMathematically, we begin with the log-likelihood function for independent and identically distributed observations from a Poisson distribution:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log(\\lambda) - \\lambda - \\log(Y_i!) \\right)\n= \\left( \\sum Y_i \\right) \\log(\\lambda) - n\\lambda - \\sum \\log(Y_i!)\n\\]\n\nTaking the derivative of the log-likelihood with respect to \\(\\lambda\\):\n\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\frac{\\sum Y_i}{\\lambda} - n\n\\]\n\nSetting this equal to zero and solving for \\(\\lambda\\):\n\n\\[\n\\frac{\\sum Y_i}{\\lambda} = n \\quad \\Rightarrow \\quad \\hat{\\lambda}_{\\text{MLE}} = \\frac{\\sum Y_i}{n} = \\bar{Y}\n\\]\n\nThis tells us that the MLE of \\(\\lambda\\) is simply the sample mean \\(\\bar{Y}\\), which is intuitive since \\(\\lambda\\) represents the expected count in a Poisson distribution.\n\nWhile the analytical solution provides an exact MLE for \\(\\lambda\\), we now turn to a numerical approach by optimizing the log-likelihood function using optim() in R.\n\nneg_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(Inf)\n  n &lt;- length(Y)\n  -(-n * lambda + sum(Y) * log(lambda) - sum(lgamma(Y + 1)))\n}\n\nresult &lt;- optim(par = 1, fn = neg_loglikelihood, Y = Y, method = \"Brent\", lower = 0.001, upper = 20)\n\nlambda_mle &lt;- result$par\ncat(\"MLE for lambda:\", lambda_mle, \"\\n\")\n\nMLE for lambda: 3.684667 \n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nTo model variation in patent counts as a function of firm characteristics, we extend the Poisson model to include covariates. We update our log-likelihood function to depend on a parameter vector \\(\\beta\\), with \\(\\lambda_i = e^{X_i'\\beta}\\) to ensure positivity.\n\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta                     \n  lambda &lt;- exp(eta)                   \n  log_lik &lt;- sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n  \n  return(-log_lik)\n}\n\nWith the Poisson regression log-likelihood function defined, we now estimate the model parameters by optimizing the function using optim() in R. We include a constant term and relevant covariates—age, age squared, region dummies, and customer status—and use the resulting Hessian matrix to compute standard errors for the estimated coefficients.\n\ndf$age2 &lt;- df$age^2\ndf$region &lt;- factor(df$region)\nX &lt;- model.matrix(~ age + age2 + region + iscustomer, data = df)\n\n\ninit_beta &lt;- rep(0, ncol(X)) \n\nfit &lt;- optim(par = init_beta,\n             fn = poisson_regression_loglikelihood,\n             Y = Y, X = X,\n             method = \"BFGS\", hessian = TRUE)\n\n\nbeta_hat &lt;- fit$par\nhessian &lt;- fit$hessian\nvar_cov_matrix &lt;- solve(hessian) \nstd_errors &lt;- sqrt(diag(var_cov_matrix))\n\nresults &lt;- data.frame(\n  Coefficient = beta_hat,\n  Std_Error = std_errors,\n  row.names = colnames(X)\n)\n\nprint(round(results, 4))\n\n                Coefficient Std_Error\n(Intercept)         -0.1257    0.1122\nage                  0.1158    0.0064\nage2                -0.0022    0.0001\nregionNortheast     -0.0246    0.0434\nregionNorthwest     -0.0348    0.0529\nregionSouth         -0.0054    0.0524\nregionSouthwest     -0.0378    0.0472\niscustomer           0.0607    0.0321\n\n\nTo validate our custom MLE implementation, we now fit the same Poisson regression model using R’s built-in glm() function with a log link and compare the estimated coefficients and standard errors.\n\npoisson_model &lt;- glm(patents ~ age + age2 + region + iscustomer,\n                     family = poisson(link = \"log\"),\n                     data = df)\nsummary(poisson_model)\n\n\nCall:\nglm(formula = patents ~ age + age2 + region + iscustomer, family = poisson(link = \"log\"), \n    data = df)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nage2            -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThere are some difference between two methods. The custom MLE estimates obtained using optim() differ from the results produced by R’s built-in glm() function. This difference likely arises from issues related to how the model is specified or optimized. Specifically, glm() handles categorical variable encoding (such as dummy-coding for regions), intercepts, starting values, and optimization convergence using routines specifically designed for generalized linear models. These features make glm() highly robust and reliable.\n\nWhile the iscustomer coefficient in the Poisson model is statistically significant, its interpretation on the log scale makes it less intuitive. To better understand the practical impact of Blueprinty’s software, we simulate counterfactual scenarios by predicting the number of patents for each firm assuming they were a customer versus not. This allows us to estimate the average treatment effect of being a Blueprinty customer.\n\nmodel &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n             family = poisson(link = \"log\"), data = df)\ndata_0 &lt;- df\ndata_1 &lt;- df\n\ndata_0$iscustomer &lt;- 0\ndata_1$iscustomer &lt;- 1\n\ny_pred_0 &lt;- predict(model, newdata = data_0, type = \"response\")\ny_pred_1 &lt;- predict(model, newdata = data_1, type = \"response\")\n\neffect &lt;- mean(y_pred_1 - y_pred_0)\ncat(\"Average estimated effect of being a customer on patents:\", round(effect, 4), \"\\n\")\n\nAverage estimated effect of being a customer on patents: 0.7928 \n\n\n\nBased on the model, the average estimated effect of being a Blueprinty customer is approximately 0.79 additional patents per firm, holding all other firm characteristics constant. This suggests that, on average, Blueprinty customers are expected to have nearly one more patent than non-customers with similar profiles."
  },
  {
    "objectID": "blog/Project 2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/Project 2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\ndf &lt;- read.csv(\"blueprinty.csv\")\nhead(df)\n\n  patents    region  age iscustomer\n1       0   Midwest 32.5          0\n2       3 Southwest 37.5          0\n3       4 Northwest 27.0          1\n4       3 Northeast 24.5          0\n5       3 Southwest 37.0          0\n6       6 Northeast 29.5          1\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(position = \"dodge\",bins = 30) +\n  labs(title = \"Histogram of Patents by Customer Status\",\n       x = \"Number of Patents\", fill = \"Customer\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\naggregate(patents ~ iscustomer, data = df, mean)\n\n  iscustomer  patents\n1          0 3.473013\n2          1 4.133056\n\n\n\nThe histogram shows a right-skewed distribution of patents for both Blueprinty customers and non-customers, with the majority of firms holding between 0 and 6 patents. However, the distribution for customers appears slightly shifted to the right, suggesting that they tend to have more patents on average. Non-customers have an average of 3.47 patents.Customers have a higher average of 4.13 patents. This difference suggests that Blueprinty customers tend to have more patents than non-customers. However, because customers are not randomly assigned, this observed difference may be influenced by systematic differences in firm characteristics—such as age or region. A more rigorous model (e.g., regression) is necessary to isolate the potential causal effect of using Blueprinty’s software.\n\nTo better understand potential selection bias, we now examine how firm characteristics—specifically age and regional location—differ by customer status. This helps us assess whether Blueprinty customers are systematically different from non-customers.\n\nggplot(df, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +\n  geom_boxplot() +\n  labs(title = \"Firm Age by Customer Status\",\n       x = \"Customer (0 = No, 1 = Yes)\", y = \"Age\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\naggregate(age ~ iscustomer, data = df, mean)\n\n  iscustomer      age\n1          0 26.10157\n2          1 26.90021\n\n\n\nAge The boxplot shows that Blueprinty customers and non-customers have similar age distributions, though customers tend to be slightly older. The average age of customers is 26.90 years, while non-customers average 26.10 years. This suggests that customers are marginally more established on average.\n\n\nggplot(df, aes(x = region, fill = factor(iscustomer))) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Region Distribution by Customer Status\",\n       x = \"Region\", y = \"Proportion\", fill = \"Customer\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nprop.table(table(df$region, df$iscustomer), 1)\n\n           \n                    0         1\n  Midwest   0.8348214 0.1651786\n  Northeast 0.4542429 0.5457571\n  Northwest 0.8449198 0.1550802\n  South     0.8167539 0.1832461\n  Southwest 0.8249158 0.1750842\n\n\n\nRegion The stacked bar plot and proportion table reveal notable differences in regional composition: In the Northeast, over 54% of firms are Blueprinty customers — the only region where customers outnumber non-customers. In all other regions (Midwest, Northwest, South, Southwest), customers make up roughly 15%–18% of firms.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nFormula for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\): \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n\\[\nP(Y_i = y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\n\\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + y_i \\log \\lambda - \\log y_i! \\right)\n\\] To estimate the Poisson model, we begin by expressing its likelihood mathematically and implementing the corresponding log-likelihood function in R.\n\npoisson_loglikelihood &lt;- function(lambda, Y){\n  if (lambda &lt;= 0) {\n    return(-Inf)  \n  }\n  n &lt;- length(Y)\n  log_likelihood &lt;- -n * lambda + sum(Y) * log(lambda) - sum(lgamma(Y + 1))\n  return(log_likelihood)\n}\n\nWith the log-likelihood function defined, we now visualize how it behaves across a range of λ values using the observed patent counts as input.\n\nY &lt;- df$patents\n\n\nlambda_values &lt;- seq(0.1, 10, length.out = 200)\nlog_likelihoods &lt;- sapply(lambda_values, poisson_loglikelihood, Y = Y)\n\nplot(lambda_values, log_likelihoods, type = \"l\",\n     xlab = \"Lambda\", ylab = \"Log-Likelihood\",\n     main = \"Poisson Log-Likelihood as a Function of Lambda\")\n\n\n\n\n\n\n\n\nMathematically, we begin with the log-likelihood function for independent and identically distributed observations from a Poisson distribution:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log(\\lambda) - \\lambda - \\log(Y_i!) \\right)\n= \\left( \\sum Y_i \\right) \\log(\\lambda) - n\\lambda - \\sum \\log(Y_i!)\n\\]\n\nTaking the derivative of the log-likelihood with respect to \\(\\lambda\\):\n\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = \\frac{\\sum Y_i}{\\lambda} - n\n\\]\n\nSetting this equal to zero and solving for \\(\\lambda\\):\n\n\\[\n\\frac{\\sum Y_i}{\\lambda} = n \\quad \\Rightarrow \\quad \\hat{\\lambda}_{\\text{MLE}} = \\frac{\\sum Y_i}{n} = \\bar{Y}\n\\]\n\nThis tells us that the MLE of \\(\\lambda\\) is simply the sample mean \\(\\bar{Y}\\), which is intuitive since \\(\\lambda\\) represents the expected count in a Poisson distribution.\n\nWhile the analytical solution provides an exact MLE for \\(\\lambda\\), we now turn to a numerical approach by optimizing the log-likelihood function using optim() in R.\n\nneg_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(Inf)\n  n &lt;- length(Y)\n  -(-n * lambda + sum(Y) * log(lambda) - sum(lgamma(Y + 1)))\n}\n\nresult &lt;- optim(par = 1, fn = neg_loglikelihood, Y = Y, method = \"Brent\", lower = 0.001, upper = 20)\n\nlambda_mle &lt;- result$par\ncat(\"MLE for lambda:\", lambda_mle, \"\\n\")\n\nMLE for lambda: 3.684667 \n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nTo model variation in patent counts as a function of firm characteristics, we extend the Poisson model to include covariates. We update our log-likelihood function to depend on a parameter vector \\(\\beta\\), with \\(\\lambda_i = e^{X_i'\\beta}\\) to ensure positivity.\n\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta                     \n  lambda &lt;- exp(eta)                   \n  log_lik &lt;- sum(Y * log(lambda) - lambda - lgamma(Y + 1))\n  \n  return(-log_lik)\n}\n\nWith the Poisson regression log-likelihood function defined, we now estimate the model parameters by optimizing the function using optim() in R. We include a constant term and relevant covariates—age, age squared, region dummies, and customer status—and use the resulting Hessian matrix to compute standard errors for the estimated coefficients.\n\ndf$age2 &lt;- df$age^2\ndf$region &lt;- factor(df$region)\nX &lt;- model.matrix(~ age + age2 + region + iscustomer, data = df)\n\n\ninit_beta &lt;- rep(0, ncol(X)) \n\nfit &lt;- optim(par = init_beta,\n             fn = poisson_regression_loglikelihood,\n             Y = Y, X = X,\n             method = \"BFGS\", hessian = TRUE)\n\n\nbeta_hat &lt;- fit$par\nhessian &lt;- fit$hessian\nvar_cov_matrix &lt;- solve(hessian) \nstd_errors &lt;- sqrt(diag(var_cov_matrix))\n\nresults &lt;- data.frame(\n  Coefficient = beta_hat,\n  Std_Error = std_errors,\n  row.names = colnames(X)\n)\n\nprint(round(results, 4))\n\n                Coefficient Std_Error\n(Intercept)         -0.1257    0.1122\nage                  0.1158    0.0064\nage2                -0.0022    0.0001\nregionNortheast     -0.0246    0.0434\nregionNorthwest     -0.0348    0.0529\nregionSouth         -0.0054    0.0524\nregionSouthwest     -0.0378    0.0472\niscustomer           0.0607    0.0321\n\n\nTo validate our custom MLE implementation, we now fit the same Poisson regression model using R’s built-in glm() function with a log link and compare the estimated coefficients and standard errors.\n\npoisson_model &lt;- glm(patents ~ age + age2 + region + iscustomer,\n                     family = poisson(link = \"log\"),\n                     data = df)\nsummary(poisson_model)\n\n\nCall:\nglm(formula = patents ~ age + age2 + region + iscustomer, family = poisson(link = \"log\"), \n    data = df)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nage2            -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThere are some difference between two methods. The custom MLE estimates obtained using optim() differ from the results produced by R’s built-in glm() function. This difference likely arises from issues related to how the model is specified or optimized. Specifically, glm() handles categorical variable encoding (such as dummy-coding for regions), intercepts, starting values, and optimization convergence using routines specifically designed for generalized linear models. These features make glm() highly robust and reliable.\n\nWhile the iscustomer coefficient in the Poisson model is statistically significant, its interpretation on the log scale makes it less intuitive. To better understand the practical impact of Blueprinty’s software, we simulate counterfactual scenarios by predicting the number of patents for each firm assuming they were a customer versus not. This allows us to estimate the average treatment effect of being a Blueprinty customer.\n\nmodel &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n             family = poisson(link = \"log\"), data = df)\ndata_0 &lt;- df\ndata_1 &lt;- df\n\ndata_0$iscustomer &lt;- 0\ndata_1$iscustomer &lt;- 1\n\ny_pred_0 &lt;- predict(model, newdata = data_0, type = \"response\")\ny_pred_1 &lt;- predict(model, newdata = data_1, type = \"response\")\n\neffect &lt;- mean(y_pred_1 - y_pred_0)\ncat(\"Average estimated effect of being a customer on patents:\", round(effect, 4), \"\\n\")\n\nAverage estimated effect of being a customer on patents: 0.7928 \n\n\n\nBased on the model, the average estimated effect of being a Blueprinty customer is approximately 0.79 additional patents per firm, holding all other firm characteristics constant. This suggests that, on average, Blueprinty customers are expected to have nearly one more patent than non-customers with similar profiles."
  },
  {
    "objectID": "blog/Project 2/hw2_questions.html#airbnb-case-study",
    "href": "blog/Project 2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nTo explore what drives booking activity on Airbnb, we use the number of reviews as a proxy for the number of bookings. We begin by performing exploratory data analysis, cleaning the dataset to handle missing values, and then fitting a Poisson regression model to explain variation in review counts based on listing characteristics.\n\nBelow is a overview of the data\n\n\ndata &lt;- read.csv(\"airbnb.csv\")\nhead(data)\n\n  X   id days last_scraped host_since       room_type bathrooms bedrooms price\n1 1 2515 3130     4/2/2017   9/6/2008    Private room         1        1    59\n2 2 2595 3127     4/2/2017   9/9/2008 Entire home/apt         1        0   230\n3 3 3647 3050     4/2/2017 11/25/2008    Private room         1        1   150\n4 4 3831 3038     4/2/2017  12/7/2008 Entire home/apt         1        1    89\n5 5 4611 3012     4/2/2017   1/2/2009    Private room        NA        1    39\n6 6 5099 2981     4/2/2017   2/2/2009 Entire home/apt         1        1   212\n  number_of_reviews review_scores_cleanliness review_scores_location\n1               150                         9                      9\n2                20                         9                     10\n3                 0                        NA                     NA\n4               116                         9                      9\n5                93                         9                      8\n6                60                         9                      9\n  review_scores_value instant_bookable\n1                   9                f\n2                   9                f\n3                  NA                f\n4                   9                f\n5                   9                t\n6                   9                f\n\n\n\nCreates a new data frame data_clean by selecting only the relevant variables. Uses na.omit() to remove any rows that have missing values in those columns. This ensures a clean dataset for modeling.\n\n\ndata_clean &lt;- na.omit(data[, c(\"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\")])\n\n\nTransforms the instant_bookable column into a binary numeric variable: 1 if “t” (true), 0 otherwise (i.e., “f”). This makes the variable suitable for use in regression models.\n\n\ndata_clean$instant_bookable &lt;- ifelse(data_clean$instant_bookable == \"t\", 1, 0)\n\n\nhead(data_clean)\n\n  number_of_reviews days       room_type bathrooms bedrooms price\n1               150 3130    Private room         1        1    59\n2                20 3127 Entire home/apt         1        0   230\n4               116 3038 Entire home/apt         1        1    89\n6                60 2981 Entire home/apt         1        1   212\n7                60 2981 Entire home/apt         1        2   250\n9                53 2980 Entire home/apt         1        1   129\n  review_scores_cleanliness review_scores_location review_scores_value\n1                         9                      9                   9\n2                         9                     10                   9\n4                         9                      9                   9\n6                         9                      9                   9\n7                        10                      9                  10\n9                         9                     10                   9\n  instant_bookable\n1                0\n2                0\n4                0\n6                0\n7                0\n9                0\n\n\n\nFits a Poisson regression model where the outcome is number_of_reviews (proxy for bookings), and predictors include listing age (days), listing features (room type, bathrooms, bedrooms, price), review scores (cleanliness, location, value), and instant_bookable. Specifies the Poisson family with a log link, which is standard for count data.\n\n\nmodel &lt;- glm(number_of_reviews ~ days + room_type + bathrooms + bedrooms + price +\n             review_scores_cleanliness + review_scores_location + review_scores_value +\n             instant_bookable,\n             family = poisson(link = \"log\"), data = data_clean)\n\nsummary(model)\n\n\nCall:\nglm(formula = number_of_reviews ~ days + room_type + bathrooms + \n    bedrooms + price + review_scores_cleanliness + review_scores_location + \n    review_scores_value + instant_bookable, family = poisson(link = \"log\"), \n    data = data_clean)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.498e+00  1.609e-02 217.396  &lt; 2e-16 ***\ndays                       5.072e-05  3.909e-07 129.757  &lt; 2e-16 ***\nroom_typePrivate room     -1.054e-02  2.738e-03  -3.847 0.000119 ***\nroom_typeShared room      -2.463e-01  8.620e-03 -28.578  &lt; 2e-16 ***\nbathrooms                 -1.177e-01  3.749e-03 -31.394  &lt; 2e-16 ***\nbedrooms                   7.409e-02  1.992e-03  37.197  &lt; 2e-16 ***\nprice                     -1.791e-05  8.327e-06  -2.151 0.031485 *  \nreview_scores_cleanliness  1.131e-01  1.496e-03  75.611  &lt; 2e-16 ***\nreview_scores_location    -7.690e-02  1.609e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.108e-02  1.804e-03 -50.490  &lt; 2e-16 ***\ninstant_bookable           3.459e-01  2.890e-03 119.666  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 926886  on 30149  degrees of freedom\nAIC: 1048375\n\nNumber of Fisher Scoring iterations: 9\n\n\n\nThe model suggests that cleanliness, bedrooms, and instant booking availability are strong drivers of review volume. Conversely, shared room types, more bathrooms, and possibly higher prices tend to be associated with lower review counts. It depends on the coefficients of the predictors. If the coefficients are positive, they lead to more reviews."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/Project 1/A Replication of Karlan and List (2007).html",
    "href": "blog/Project 1/A Replication of Karlan and List (2007).html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study found that mentioning a matching grant increased both the probability of donation and the amount given, but increasing the match ratio beyond $1:1 did not further increase giving. These results suggest that the presence of a match signal matters more than its magnitude. Interestingly, the effect was stronger in politically conservative (“red”) states, suggesting that local context may influence the impact of fundraising appeals.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project 1/A Replication of Karlan and List (2007).html#introduction",
    "href": "blog/Project 1/A Replication of Karlan and List (2007).html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study found that mentioning a matching grant increased both the probability of donation and the amount given, but increasing the match ratio beyond $1:1 did not further increase giving. These results suggest that the presence of a match signal matters more than its magnitude. Interestingly, the effect was stronger in politically conservative (“red”) states, suggesting that local context may influence the impact of fundraising appeals.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project 1/A Replication of Karlan and List (2007).html#data",
    "href": "blog/Project 1/A Replication of Karlan and List (2007).html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nlibrary(haven)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n\ndf &lt;- read_dta(\"karlan_list_2007.dta\")\n\n\ndf\n\n# A tibble: 50,083 × 51\n   treatment control ratio    ratio2 ratio3 size    size25 size50 size100 sizeno\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl+lb&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl+l&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 2         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 3         1       0 1             0      0 3 [$10…      0      0       1      0\n 4         1       0 1             0      0 4 [Uns…      0      0       0      1\n 5         1       0 1             0      0 2 [$50…      0      1       0      0\n 6         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 7         1       0 1             0      0 1 [$25…      1      0       0      0\n 8         1       0 2             1      0 3 [$10…      0      0       1      0\n 9         1       0 2             1      0 4 [Uns…      0      0       0      1\n10         1       0 1             0      0 1 [$25…      1      0       0      0\n# ℹ 50,073 more rows\n# ℹ 41 more variables: ask &lt;dbl+lbl&gt;, askd1 &lt;dbl&gt;, askd2 &lt;dbl&gt;, askd3 &lt;dbl&gt;,\n#   ask1 &lt;dbl&gt;, ask2 &lt;dbl&gt;, ask3 &lt;dbl&gt;, amount &lt;dbl&gt;, gave &lt;dbl&gt;,\n#   amountchange &lt;dbl&gt;, hpa &lt;dbl&gt;, ltmedmra &lt;dbl&gt;, freq &lt;dbl&gt;, years &lt;dbl&gt;,\n#   year5 &lt;dbl&gt;, mrm2 &lt;dbl&gt;, dormant &lt;dbl&gt;, female &lt;dbl&gt;, couple &lt;dbl&gt;,\n#   state50one &lt;dbl&gt;, nonlit &lt;dbl&gt;, cases &lt;dbl&gt;, statecnt &lt;dbl&gt;,\n#   stateresponse &lt;dbl&gt;, stateresponset &lt;dbl&gt;, stateresponsec &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nSummarize numerical columns & frequency of treatment groups\n\n\n\n\n\n\n# Summarize numerical columns\nsummary(df)\n\n   treatment         control           ratio           ratio2      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.6668   Mean   :0.3332   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     ratio3            size           size25           size50      \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :2.000   Median :0.0000   Median :0.0000  \n Mean   :0.2222   Mean   :1.667   Mean   :0.1667   Mean   :0.1666  \n 3rd Qu.:0.0000   3rd Qu.:3.000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :4.000   Max.   :1.0000   Max.   :1.0000  \n                                                                   \n    size100           sizeno            ask            askd1       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.1667   Mean   :0.1667   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     askd2            askd3             ask1             ask2        \n Min.   :0.0000   Min.   :0.0000   Min.   :  25.0   Min.   :  35.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:  35.0   1st Qu.:  45.00  \n Median :0.0000   Median :0.0000   Median :  45.0   Median :  60.00  \n Mean   :0.2223   Mean   :0.2222   Mean   :  71.5   Mean   :  91.79  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:  65.0   3rd Qu.:  85.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1500.0   Max.   :1875.00  \n                                                                     \n      ask3          amount              gave          amountchange       \n Min.   :  50   Min.   :  0.0000   Min.   :0.00000   Min.   :-200412.12  \n 1st Qu.:  55   1st Qu.:  0.0000   1st Qu.:0.00000   1st Qu.:    -50.00  \n Median :  70   Median :  0.0000   Median :0.00000   Median :    -30.00  \n Mean   : 111   Mean   :  0.9157   Mean   :0.02065   Mean   :    -52.67  \n 3rd Qu.: 100   3rd Qu.:  0.0000   3rd Qu.:0.00000   3rd Qu.:    -25.00  \n Max.   :2250   Max.   :400.0000   Max.   :1.00000   Max.   :    275.00  \n                                                                         \n      hpa             ltmedmra           freq             years       \n Min.   :   0.00   Min.   :0.0000   Min.   :  0.000   Min.   : 0.000  \n 1st Qu.:  30.00   1st Qu.:0.0000   1st Qu.:  2.000   1st Qu.: 2.000  \n Median :  45.00   Median :0.0000   Median :  4.000   Median : 5.000  \n Mean   :  59.38   Mean   :0.4937   Mean   :  8.039   Mean   : 6.098  \n 3rd Qu.:  60.00   3rd Qu.:1.0000   3rd Qu.: 10.000   3rd Qu.: 9.000  \n Max.   :1000.00   Max.   :1.0000   Max.   :218.000   Max.   :95.000  \n                                                      NA's   :1       \n     year5             mrm2           dormant           female      \n Min.   :0.0000   Min.   :  0.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:  4.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :  8.00   Median :1.0000   Median :0.0000  \n Mean   :0.5088   Mean   : 13.01   Mean   :0.5235   Mean   :0.2777  \n 3rd Qu.:1.0000   3rd Qu.: 19.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :168.00   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :1                         NA's   :1111    \n     couple         state50one            nonlit          cases    \n Min.   :0.0000   Min.   :0.0000000   Min.   :0.000   Min.   :0.0  \n 1st Qu.:0.0000   1st Qu.:0.0000000   1st Qu.:1.000   1st Qu.:1.0  \n Median :0.0000   Median :0.0000000   Median :3.000   Median :1.0  \n Mean   :0.0919   Mean   :0.0009983   Mean   :2.474   Mean   :1.5  \n 3rd Qu.:0.0000   3rd Qu.:0.0000000   3rd Qu.:4.000   3rd Qu.:2.0  \n Max.   :1.0000   Max.   :1.0000000   Max.   :6.000   Max.   :4.0  \n NA's   :1148                         NA's   :452     NA's   :452  \n    statecnt         stateresponse     stateresponset    stateresponsec   \n Min.   : 0.001995   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.: 1.833234   1st Qu.:0.01816   1st Qu.:0.01849   1st Qu.:0.01286  \n Median : 3.538799   Median :0.01971   Median :0.02170   Median :0.01988  \n Mean   : 5.998820   Mean   :0.02063   Mean   :0.02199   Mean   :0.01772  \n 3rd Qu.: 9.607021   3rd Qu.:0.02305   3rd Qu.:0.02470   3rd Qu.:0.02081  \n Max.   :17.368841   Max.   :0.07692   Max.   :0.11111   Max.   :0.05263  \n                                                         NA's   :3        \n stateresponsetminc     perbush           close25            red0       \n Min.   :-0.047619   Min.   :0.09091   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:-0.001388   1st Qu.:0.44444   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 0.001779   Median :0.48485   Median :0.0000   Median :0.0000  \n Mean   : 0.004273   Mean   :0.48794   Mean   :0.1857   Mean   :0.4045  \n 3rd Qu.: 0.010545   3rd Qu.:0.52525   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   : 0.111111   Max.   :0.73196   Max.   :1.0000   Max.   :1.0000  \n NA's   :3           NA's   :35        NA's   :35       NA's   :35      \n     blue0            redcty          bluecty           pwhite       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00942  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.75584  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :0.87280  \n Mean   :0.5955   Mean   :0.5102   Mean   :0.4887   Mean   :0.81960  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.93883  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n NA's   :35       NA's   :105      NA's   :105      NA's   :1866     \n     pblack          page18_39        ave_hh_sz     median_hhincome \n Min.   :0.00000   Min.   :0.0000   Min.   :0.000   Min.   :  5000  \n 1st Qu.:0.01473   1st Qu.:0.2583   1st Qu.:2.210   1st Qu.: 39181  \n Median :0.03655   Median :0.3055   Median :2.440   Median : 50673  \n Mean   :0.08671   Mean   :0.3217   Mean   :2.429   Mean   : 54816  \n 3rd Qu.:0.09088   3rd Qu.:0.3691   3rd Qu.:2.660   3rd Qu.: 66005  \n Max.   :0.98962   Max.   :0.9975   Max.   :5.270   Max.   :200001  \n NA's   :2036      NA's   :1866     NA's   :1862    NA's   :1874    \n     powner        psch_atlstba    pop_propurban   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.5602   1st Qu.:0.2356   1st Qu.:0.8849  \n Median :0.7123   Median :0.3737   Median :1.0000  \n Mean   :0.6694   Mean   :0.3917   Mean   :0.8720  \n 3rd Qu.:0.8168   3rd Qu.:0.5300   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :1869     NA's   :1868     NA's   :1866    \n\n# Frequency of treatment groups\ntable(df$treatment, df$ratio)\n\n   \n        0     1     2     3\n  0 16687     0     0     0\n  1     0 11133 11134 11129\n\n\n\n\n\nThe dataset consists of 50,083 observations collected from a large-scale experiment on charitable giving. Each row represents a potential donor who received a fundraising letter as part of the experiment. The primary treatment variable (treatment) indicates whether the recipient was offered a matching grant, while the ratio variable specifies the match rate ($1:$1, $2:$1, or $3:$1). About 67% of participants received a match treatment, evenly split across the three match ratios. The dataset also includes variations in the maximum match amount (size) and suggested donation levels (ask1, ask2, ask3). Key outcome variables include whether a donation was made (gave) and the donation amount (amount), which has a highly skewed distribution with a mean of $0.92 and a maximum of $400. Additional variables describe donor giving history (e.g., hpa for highest previous amount), demographics (e.g., female, couple), and ZIP-level census attributes such as income, education, and urbanization. Political context is captured by indicators like redcty, bluecty, and perbush, allowing for analysis of heterogeneous treatment effects by political leaning.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# Variables to summarize\nvars &lt;- c(\n  \"mrm2\", \"hpa\", \"freq\", \"years\", \"dormant\", \"female\", \"couple\",\n  \"pwhite\", \"pblack\", \"page18_39\", \"ave_hh_sz\", \"red0\", \"redcty\",\n  \"nonlit\", \"cases\"\n)\n\n# Function to compute mean and sd\nsummarize_stats &lt;- function(df, var) {\n  m &lt;- mean(df[[var]], na.rm = TRUE)\n  s &lt;- sd(df[[var]], na.rm = TRUE)\n  n &lt;- sum(!is.na(df[[var]]))\n  sprintf(\"%.3f (%.3f)\", m, s)\n}\n\n\n# Prepare summary table\nsummary_table &lt;- data.frame(\n  Variable = vars,\n  All = sapply(vars, function(v) summarize_stats(df, v)),\n  Treatment = sapply(vars, function(v) summarize_stats(df[df$treatment == 1, ], v)),\n  Control = sapply(vars, function(v) summarize_stats(df[df$treatment == 0, ], v))\n)\n\n# View summary\nsummary_table\n\n           Variable             All       Treatment         Control\nmrm2           mrm2 13.007 (12.081) 13.012 (12.086) 12.998 (12.074)\nhpa             hpa 59.385 (71.177) 59.597 (73.052) 58.960 (67.269)\nfreq           freq  8.039 (11.394)  8.035 (11.390)  8.047 (11.404)\nyears         years   6.098 (5.503)   6.078 (5.442)   6.136 (5.625)\ndormant     dormant   0.523 (0.499)   0.524 (0.499)   0.523 (0.499)\nfemale       female   0.278 (0.448)   0.275 (0.447)   0.283 (0.450)\ncouple       couple   0.092 (0.289)   0.091 (0.288)   0.093 (0.290)\npwhite       pwhite   0.820 (0.169)   0.819 (0.168)   0.820 (0.169)\npblack       pblack   0.087 (0.136)   0.087 (0.135)   0.087 (0.137)\npage18_39 page18_39   0.322 (0.103)   0.322 (0.103)   0.322 (0.103)\nave_hh_sz ave_hh_sz   2.429 (0.378)   2.430 (0.378)   2.427 (0.379)\nred0           red0   0.404 (0.491)   0.407 (0.491)   0.399 (0.490)\nredcty       redcty   0.510 (0.500)   0.512 (0.500)   0.507 (0.500)\nnonlit       nonlit   2.474 (1.962)   2.485 (1.966)   2.453 (1.953)\ncases         cases   1.500 (1.155)   1.499 (1.157)   1.502 (1.152)\n\n\n\n# Store results\nsummary_results &lt;- data.frame()\n\nfor (v in vars) {\n  df_clean &lt;- df[!is.na(df[[v]]), ]\n  \n  # Means, SDs, Ns\n  xA &lt;- mean(df_clean[[v]][df_clean$treatment == 1])\n  xB &lt;- mean(df_clean[[v]][df_clean$treatment == 0])\n  sA2 &lt;- var(df_clean[[v]][df_clean$treatment == 1])\n  sB2 &lt;- var(df_clean[[v]][df_clean$treatment == 0])\n  nA &lt;- sum(df_clean$treatment == 1)\n  nB &lt;- sum(df_clean$treatment == 0)\n  \n  # Manual t-stat from class formula\n  t_manual &lt;- (xA - xB) / sqrt(sA2/nA + sB2/nB)\n  \n  # Linear regression\n  reg &lt;- lm(as.formula(paste(v, \"~ treatment\")), data = df_clean)\n  reg_summary &lt;- summary(reg)\n  coef_est &lt;- reg_summary$coefficients[\"treatment\", \"Estimate\"]\n  coef_se  &lt;- reg_summary$coefficients[\"treatment\", \"Std. Error\"]\n  reg_t    &lt;- reg_summary$coefficients[\"treatment\", \"t value\"]\n  reg_p    &lt;- reg_summary$coefficients[\"treatment\", \"Pr(&gt;|t|)\"]\n  \n  summary_results &lt;- rbind(summary_results, data.frame(\n    Variable = v,\n    T_Manual = round(t_manual, 3),\n    Reg_t = round(reg_t, 3),\n    Reg_p = round(reg_p, 3)\n  ))\n}\n\n# View comparison\nsummary_results\n\n    Variable T_Manual  Reg_t Reg_p\n1       mrm2    0.120  0.119 0.905\n2        hpa    0.970  0.944 0.345\n3       freq   -0.111 -0.111 0.912\n4      years   -1.091 -1.103 0.270\n5    dormant    0.174  0.174 0.862\n6     female   -1.754 -1.758 0.079\n7     couple   -0.582 -0.584 0.559\n8     pwhite   -0.559 -0.560 0.575\n9     pblack    0.098  0.098 0.922\n10 page18_39   -0.124 -0.124 0.901\n11 ave_hh_sz    0.823  0.824 0.410\n12      red0    1.877  1.875 0.061\n13    redcty    0.904  0.904 0.366\n14    nonlit    1.705  1.702 0.089\n15     cases   -0.341 -0.341 0.733\n\n\nAs shown in the summary table, the treatment and control groups are very similar across all variables. For example, the average highest previous donation (hpa) is $59.60 in the treatment group and $58.96 in the control group. The share of females is 27.5% in the treatment group and 28.3% in the control group. These differences are minimal and consistent with what we’d expect from random assignment.\nTo formally test for balance, I computed t-statistics both manually and using linear regressions with treatment status as the dependent variable. The t-values are small across the board, and none of the variables have statistically significant differences at conventional levels (all p-values &gt; 0.05). The closest case is the female variable with a t-statistic of -1.76 and a p-value of 0.079, which is still above the standard 5% threshold.\nThese results align closely with Table 1 in Karlan and List (2007), where they also demonstrate covariate balance between treatment arms. This reassures me that the randomization was implemented properly, and any differences in donation outcomes between groups can be interpreted as causal effects of the treatment."
  },
  {
    "objectID": "blog/Project 1/A Replication of Karlan and List (2007).html#experimental-results",
    "href": "blog/Project 1/A Replication of Karlan and List (2007).html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\ndf %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\")) %&gt;%\n  group_by(group) %&gt;%\n  summarise(response_rate = mean(gave, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = group, y = response_rate, fill = group)) +\n  geom_col(width = 0.6) +\n  labs(title = \"Response Rate by Treatment Group\",\n       x = \"Group\",\n       y = \"Proportion Donated\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nAccording to the bar chart, more people gave when they were told their donation would be matched, even though the total increase was less than half a percentage point.\n\n# Create summary stats by treatment\ndf %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\")) %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    response_rate = round(mean(gave), 3),\n    \n    unconditional_donation = round(mean(amount), 3),\n    \n    conditional_donation = round(mean(amount[gave == 1]), 3),\n    \n    observations  = n()\n  )\n\n# A tibble: 2 × 5\n  group   response_rate unconditional_donation conditional_donation observations\n  &lt;chr&gt;           &lt;dbl&gt;                  &lt;dbl&gt;                &lt;dbl&gt;        &lt;int&gt;\n1 Control         0.018                  0.813                 45.5        16687\n2 Treatm…         0.022                  0.967                 43.9        33396\n\n\n\n# Manual t-test for difference in response rates\nx_treat &lt;- mean(df$gave[df$treatment == 1])\nx_ctrl &lt;- mean(df$gave[df$treatment == 0])\ns2_treat &lt;- var(df$gave[df$treatment == 1])\ns2_ctrl &lt;- var(df$gave[df$treatment == 0])\nn_treat &lt;- sum(df$treatment == 1)\nn_ctrl &lt;- sum(df$treatment == 0)\n\nse_diff &lt;- sqrt(s2_treat / n_treat + s2_ctrl / n_ctrl)\nt_stat &lt;- (x_treat - x_ctrl) / se_diff\n\nse_diff\n\n[1] 0.001302509\n\nt_stat\n\n[1] 3.209462\n\n\n\nmodel &lt;- lm(gave ~ treatment, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = gave ~ treatment, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02204 -0.02204 -0.02204 -0.01786  0.98214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.017858   0.001101  16.225  &lt; 2e-16 ***\ntreatment   0.004180   0.001348   3.101  0.00193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1422 on 50081 degrees of freedom\nMultiple R-squared:  0.000192,  Adjusted R-squared:  0.0001721 \nF-statistic: 9.618 on 1 and 50081 DF,  p-value: 0.001927\n\n\nI first compared response rates between the treatment group and the control group. The response rate was 2.2% for the treatment group compared to 1.8% in the control group. Next, I estimated a simple linear probability model where the outcome was whether someone donated (gave), and the only predictor was whether they were in the treatment group. The regression shows that being in the treatment group increased the probability of giving by 0.48 percentage points, and this difference is statistically significant (p = 0.0019). The corresponding t-statistic (3.2) confirms this effect is unlikely to have occurred by chance.\nIn more intuitive terms: people who received a fundraising letter with a matching grant offer were modestly but significantly more likely to donate. Interestingly, although more people donated in the treatment group, the average donation amount among those who gave was slightly lower ($43.87 vs. $45.54). This suggests that the match offer encouraged more people to participate, possibly including those who were willing to give smaller amounts.\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nlibrary(margins)\n\n# Probit regression: same as Table 3, Column 1\nprobit_model &lt;- glm(gave ~ treatment, data = df, family = binomial(link = \"probit\"))\n\n# Show regression output\nsummary(probit_model)\n\n\nCall:\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.10014    0.02332 -90.074  &lt; 2e-16 ***\ntreatment    0.08678    0.02788   3.113  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10071  on 50082  degrees of freedom\nResidual deviance: 10061  on 50081  degrees of freedom\nAIC: 10065\n\nNumber of Fisher Scoring iterations: 6\n\n# Calculate marginal effect of treatment\nmfx &lt;- margins(probit_model)\nsummary(mfx)\n\n    factor    AME     SE      z      p  lower  upper\n treatment 0.0043 0.0014 3.1044 0.0019 0.0016 0.0070\n\n\nBased on the probit regression, Estimate = 0.08678, p = 0.0018, This means that this effect is statistically significant at the 1% level. To make the probit model easier to interpret, I calculated the Average Marginal Effect (AME) of treatment: AME = 0.0043, This means that offering a matched donation increases the probability of giving by about 0.43 percentage points, on average, holding everything else constant. The result is statistically significant.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Filter for treatment groups only (exclude control)\ndf_treat &lt;- df %&gt;% filter(treatment == 1)\n\n# Make sure ratio is a factor for grouping\ndf_treat &lt;- df_treat %&gt;% mutate(ratio = as.character(ratio))\n\n# 1. Compare 2:1 vs. 1:1\ngave_2_1 &lt;- df_treat$gave[df_treat$ratio == \"2\"]\ngave_1_1 &lt;- df_treat$gave[df_treat$ratio == \"1\"]\n\nt_2_vs_1 &lt;- t.test(gave_2_1, gave_1_1, var.equal = TRUE)\nprint(t_2_vs_1)\n\n\n    Two Sample t-test\n\ndata:  gave_2_1 and gave_1_1\nt = 0.96505, df = 22265, p-value = 0.3345\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.001942780  0.005711282\nsample estimates:\n mean of x  mean of y \n0.02263338 0.02074912 \n\n# 2. Compare 3:1 vs. 1:1\ngave_3_1 &lt;- df_treat$gave[df_treat$ratio == \"3\"]\n\nt_3_vs_1 &lt;- t.test(gave_3_1, gave_1_1, var.equal = TRUE)\nprint(t_3_vs_1)\n\n\n    Two Sample t-test\n\ndata:  gave_3_1 and gave_1_1\nt = 1.015, df = 22260, p-value = 0.3101\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.00184747  0.00581602\nsample estimates:\n mean of x  mean of y \n0.02273340 0.02074912 \n\n# 3. Compare 3:1 vs. 2:1\nt_3_vs_2 &lt;- t.test(gave_3_1, gave_2_1, var.equal = TRUE)\nprint(t_3_vs_2)\n\n\n    Two Sample t-test\n\ndata:  gave_3_1 and gave_2_1\nt = 0.050116, df = 22261, p-value = 0.96\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.003811994  0.004012042\nsample estimates:\n mean of x  mean of y \n0.02273340 0.02263338 \n\n\nThis confirms what Karlan & List meant when they said the figures “suggest” no meaningful effect of varying the match ratio. Despite increasing the match rate from $1:$1 to $2:$1 or $3:$1, donation rates did not increase, supporting their argument that the presence of a match matters more than the size of the match.\nIn behavioral terms, it suggests that what motivates people to give is not necessarily the economic value of the match—but rather the signal that someone else is matching at all. Once that social or psychological nudge is activated, increasing the ratio does not seem to matter.\n\nmodel_ratio &lt;- lm(gave ~ factor(ratio), data = df %&gt;% filter(treatment == 1))\nsummary(model_ratio)\n\n\nCall:\nlm(formula = gave ~ factor(ratio), data = df %&gt;% filter(treatment == \n    1))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02273 -0.02273 -0.02263 -0.02075  0.97925 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.020749   0.001391  14.912   &lt;2e-16 ***\nfactor(ratio)2 0.001884   0.001968   0.958    0.338    \nfactor(ratio)3 0.001984   0.001968   1.008    0.313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1468 on 33393 degrees of freedom\nMultiple R-squared:  3.865e-05, Adjusted R-squared:  -2.124e-05 \nF-statistic: 0.6454 on 2 and 33393 DF,  p-value: 0.5245\n\n\nIntercept (0.020749): This is the estimated probability of giving for the baseline category, which in this case is the $1:$1 match group. So, about 2.07% of individuals donated in the $1:$1 condition.\nratio2 (0.001884): This is the additional effect of receiving a $2:$1 match compared to $1:$1. The donation rate is estimated to be 0.19 percentage points higher, but this is not statistically significant (p = 0.338).\nratio3 (0.001984): Similarly, this is the estimated increase in giving under a $3:$1 match vs. $1:$1. Again, the increase is about 0.20 percentage points, but not statistically significant (p = 0.313).\nAll p-values are well above 0.05, meaning the differences between match ratios are not statistically significant. The R-squared is essentially zero (~0.00004), indicating that match ratio explains virtually none of the variation in giving behavior among those offered any match. The standard errors (~0.002) are large relative to the size of the estimated effects (~0.002), meaning we lack the precision needed to distinguish these effects from zero.\n\nlibrary(tidyr)\n\n\n# Calculate raw response rates for each match ratio\ndf %&gt;%\n  filter(ratio %in% c(\"1\", \"2\", \"3\")) %&gt;%\n  group_by(ratio) %&gt;%\n  summarise(response_rate = mean(gave, na.rm = TRUE)) %&gt;%\n  pivot_wider(names_from = ratio, values_from = response_rate) -&gt; rate_table\n\n# Calculate differences\ndiff_2_vs_1 &lt;- rate_table$`2` - rate_table$`1`\ndiff_3_vs_2 &lt;- rate_table$`3` - rate_table$`2`\n\ncat(\"Response rate difference (2:1 vs 1:1):\", round(diff_2_vs_1, 4), \"\\n\")\n\nResponse rate difference (2:1 vs 1:1): 0.0019 \n\ncat(\"Response rate difference (3:1 vs 2:1):\", round(diff_3_vs_2, 4), \"\\n\")\n\nResponse rate difference (3:1 vs 2:1): 1e-04 \n\n\n\ncoef_table &lt;- summary(model_ratio)$coefficients\n\n# Extract coefficients\ncoef_1_1 &lt;- coef_table[\"(Intercept)\", \"Estimate\"]\ncoef_2_1 &lt;- coef_table[\"factor(ratio)2\", \"Estimate\"]\ncoef_3_1 &lt;- coef_table[\"factor(ratio)3\", \"Estimate\"]\n\n# Differences between fitted values\nfitted_2_vs_1 &lt;- coef_2_1\nfitted_3_vs_2 &lt;- coef_3_1 - coef_2_1\n\ncat(\"Fitted response rate difference (2:1 vs 1:1):\", round(fitted_2_vs_1, 4), \"\\n\")\n\nFitted response rate difference (2:1 vs 1:1): 0.0019 \n\ncat(\"Fitted response rate difference (3:1 vs 2:1):\", round(fitted_3_vs_2, 4), \"\\n\")\n\nFitted response rate difference (3:1 vs 2:1): 1e-04 \n\n\nBoth approaches yield the same values: - 2:1 vs 1:1 difference: +0.0019 (or 0.19 percentage points) - 3:1 vs 2:1 difference: +0.0001 (or 0.01 percentage points) These differences are very small, and as confirmed by previous t-tests and regression output, they are not statistically significant.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# Run linear regression on amount\nmodel_amount_ratio &lt;- lm(amount ~ ratio, data = df_treat)\nsummary(model_amount_ratio)\n\n\nCall:\nlm(formula = amount ~ ratio, data = df_treat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -1.03  -1.03  -0.94  -0.94 399.06 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.936675   0.084950  11.026   &lt;2e-16 ***\nratio2      0.089461   0.120135   0.745    0.456    \nratio3      0.001118   0.120149   0.009    0.993    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.963 on 33393 degrees of freedom\nMultiple R-squared:  2.187e-05, Adjusted R-squared:  -3.802e-05 \nF-statistic: 0.3651 on 2 and 33393 DF,  p-value: 0.6941\n\n\nIntercept (0.937): This is the average donation amount in the $1:$1 match group, about $0.94. ratio2 (0.089): The difference in average donation between $2:$1 and $1:$1 is $0.09, which is small and not statistically significant (p = 0.456). ratio3 (0.001): The difference between $3:$1 and $1:$1 is basically zero, and completely insignificant (p = 0.993). The R-squared is nearly zero, which confirms that the match ratio explains virtually none of the variation in how much people donated. Even though one might expect that a more generous match (like 3:1) would motivate people to give more, the data show that donation amounts remain essentially unchanged regardless of whether the match is 1:1, 2:1, or 3:1.\n\n# Filter: only people who gave (conditional on donation)\ndf_conditional &lt;- df %&gt;%\n  filter(treatment == 1, gave == 1)\n\n# Run linear regression on amount conditional on giving\nmodel_cond &lt;- lm(amount ~ ratio, data = df_conditional)\nsummary(model_cond)\n\n\nCall:\nlm(formula = amount ~ ratio, data = df_conditional)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-42.93 -23.93 -16.95   6.07 354.09 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   47.889      4.177  11.465   &lt;2e-16 ***\nratio         -1.979      1.911  -1.036    0.301    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.01 on 734 degrees of freedom\nMultiple R-squared:  0.001459,  Adjusted R-squared:  9.833e-05 \nF-statistic: 1.072 on 1 and 734 DF,  p-value: 0.3008\n\n\nIntercept (45.14): Donors in the $1:$1 match group gave $45.14 on average. ratio2 (0.19, p = 0.960): Donors in the $2:$1 group gave about $0.19 more than those in the $1:$1 group. This is not statistically significant and essentially zero. ratio3 (-3.89, p = 0.309): Donors in the $3:$1 group gave about $3.89 less than those in the $1:$1 group. Again, this is not statistically significant.\nThis analysis is conditional on a post-treatment variable: giving (gave == 1). That means the sample is selected based on behavior that might itself be affected by the treatment, which introduces selection bias. so he treatment coefficient doesn’t have causal relationship.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Filter for donors only and label groups\ndf_donated &lt;- df %&gt;%\n  filter(gave == 1) %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\"))\n\n# Calculate group means\ngroup_means &lt;- df_donated %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_donation = mean(amount), .groups = \"drop\")\n\n# Plot: Treatment group histogram\nggplot(filter(df_donated, group == \"Treatment\"), aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"white\") +\n  geom_vline(data = filter(group_means, group == \"Treatment\"),\n             aes(xintercept = mean_donation),\n             color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Donation Amounts: Treatment Group\",\n       x = \"Donation Amount ($)\",\n       y = \"Number of Donors\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n# Plot: Control group histogram\nggplot(filter(df_donated, group == \"Control\"), aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"orange\", color = \"white\") +\n  geom_vline(data = filter(group_means, group == \"Control\"),\n             aes(xintercept = mean_donation),\n             color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Donation Amounts: Control Group\",\n       x = \"Donation Amount ($)\",\n       y = \"Number of Donors\")"
  },
  {
    "objectID": "blog/Project 1/A Replication of Karlan and List (2007).html#simulation-experiment",
    "href": "blog/Project 1/A Replication of Karlan and List (2007).html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nset.seed(42)  # for reproducibility\n\n# Simulate control: p = 0.018 (no match)\ncontrol_draws &lt;- rbinom(n = 10000, size = 1, prob = 0.018)\n\n# Simulate treatment: p = 0.022 (with any match)\ntreatment_draws &lt;- rbinom(n = 10000, size = 1, prob = 0.022)\ndifference_vector &lt;- treatment_draws - control_draws\n\n# Cumulative average of the differences\ncum_avg &lt;- cumsum(difference_vector) / seq_along(difference_vector)\n\n\n# Plot it\nplot(cum_avg, type = \"l\", col = \"blue\", lwd = 2,\n     main = \"Cumulative Average of Simulated Difference in Donation Rates\",\n     xlab = \"Number of Observations\",\n     ylab = \"Cumulative Average Treatment - Control\")\nabline(h = 0.022 - 0.018, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topright\", legend = c(\"Cumulative Average\", \"True Difference (0.004)\"),\n       col = c(\"blue\", \"red\"), lty = c(1, 2), lwd = 2)\n\n\n\n\n\n\n\n\nThe cumulative average of simulate difference is close to the true mean 0.004. As sample size increases, sample averages become more stable and reliably reflect population values.\n\n\nCentral Limit Theorem\n\n# Define true probabilities\np_control &lt;- 0.018\np_treat &lt;- 0.022\n\n# Sample sizes to simulate\nsample_sizes &lt;- c(50, 200, 500, 1000)\n\n# Function to simulate sampling distribution of differences\nsimulate_diff_dist &lt;- function(n, reps = 1000) {\n  replicate(reps, {\n    control_sample &lt;- rbinom(n, size = 1, prob = p_control)\n    treat_sample   &lt;- rbinom(n, size = 1, prob = p_treat)\n    mean(treat_sample) - mean(control_sample)\n  })\n}\n\n# Simulate all four scenarios\ndiffs_list &lt;- lapply(sample_sizes, simulate_diff_dist)\n\n# Plot histograms\npar(mfrow = c(2, 2))  # 2x2 layout\n\nfor (i in 1:4) {\n  hist(diffs_list[[i]], breaks = 30,\n       main = paste(\"Sample Size =\", sample_sizes[i]),\n       xlab = \"Difference in Means (Treatment - Control)\",\n       col = \"lightblue\", border = \"white\")\n  abline(v = 0, col = \"red\", lwd = 2, lty = 2)  # Reference line at 0\n}\n\n\n\n\n\n\n\n\nSample size = 50: The distribution is wide and highly variable. The true effect is tiny (0.004), and the distribution is very spread out. Zero appears near the center, suggesting that with such a small sample size, it is easily fail to detect the true positive effect.\nSample size = 200: The distribution narrows, but still shows a good deal of spread. Zero is still roughly in the center, which again means there’s a high chance we wouldn’t reject the null in a real experiment of this size.\nSample size = 500: The distribution is now visibly tighter, with more concentration around a slightly positive mean. Zero is no longer the exact center, but it is still well within the bulk of the distribution—indicating marginal power to detect small effects.\nSample size = 1000: The distribution is narrower and more symmetric, and zero is near the edge of the central mass, moving toward the tail. This suggests that with 1,000 observations per group, it is possible to have enough precision to see that the true difference is not zero, even though the effect is small.\nAs sample size increases, the distribution of sample means becomes more normal and concentrated. With small samples, the sampling distribution is wide, and zero is in the center, meaning it’s hard to detect small effects. As sample size grows, the distribution tightens, and zero moves toward the tail—reflecting increasing power to detect a true positive effect."
  },
  {
    "objectID": "blog/Project 3/hw3_questions.html",
    "href": "blog/Project 3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/Project 3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/Project 3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/Project 3/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/Project 3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "blog/Project 3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/Project 3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nTo begin the analysis, I first prepared the conjoint dataset to ensure it was in the correct format for modeling. This involved reshaping the data, creating dummy variables for categorical attributes like brand and ad exposure, and sorting it by respondent and task. The following code shows how these preprocessing steps were carried out:\n\nconjoint_data &lt;- read.csv(\"conjoint_data.csv\")\n\n# Create dummy variables\nconjoint_data$brand_N &lt;- as.integer(conjoint_data$brand == \"N\")\nconjoint_data$brand_P &lt;- as.integer(conjoint_data$brand == \"P\")\nconjoint_data$ad_Yes  &lt;- as.integer(conjoint_data$ad == \"Yes\")\n\n# Sort by respondent and task\nconjoint_data &lt;- conjoint_data[order(conjoint_data$resp, conjoint_data$task), ]\n\nThe final dataset is:\n\nhead(conjoint_data)\n\n  resp task choice brand  ad price brand_N brand_P ad_Yes\n1    1    1      1     N Yes    28       1       0      1\n2    1    1      0     H Yes    16       0       0      1\n3    1    1      0     P Yes    16       0       1      1\n4    1    2      0     N Yes    32       1       0      1\n5    1    2      1     P Yes    16       0       1      1\n6    1    2      0     N Yes    24       1       0      1"
  },
  {
    "objectID": "blog/Project 3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/Project 3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nTo begin the estimation process, I first coded the log-likelihood function for the multinomial logit model. This function captures the probability of each choice being made, given the model parameters.\n\nlog_likelihood &lt;- function(beta, data) {\n  X &lt;- as.matrix(conjoint_data[, c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\")])\n  y &lt;- conjoint_data$choice\n  n &lt;- nrow(conjoint_data)\n  Xb &lt;- X %*% beta\n  groups &lt;- rep(1:(n / 3), each = 3)\n\n  log_lik &lt;- tapply(1:n, groups, function(idx) {\n    xb &lt;- Xb[idx]\n    chosen &lt;- y[idx]\n    \n    if (sum(chosen) != 1) return(NA)\n\n    max_xb &lt;- max(xb)\n    denom &lt;- max_xb + log(sum(exp(xb - max_xb)))\n    num &lt;- xb[which(chosen == 1)]\n\n    return(num - denom)\n  })\n\n  total &lt;- sum(unlist(log_lik), na.rm = TRUE)\n  if (is.na(total) || is.infinite(total)) return(1e6)\n  return(-total)\n}\n\nWith the log-likelihood function defined, I then used the optim() function in R to estimate the model parameters by maximizing the log-likelihood. I also computed standard errors and constructed 95% confidence intervals for interpretation.\n\ninit_beta &lt;- c(1, 0.5, -0.8, -0.1)\n\nmle &lt;- optim(\n  par = init_beta,\n  fn = log_likelihood,\n  data = conjoint_data,\n  method = \"BFGS\",\n  hessian = TRUE,\n  control = list(maxit = 1000)\n)\n\n# Check convergence\nprint(paste(\"Convergence code:\", mle$convergence))  # 0 = success\n\n[1] \"Convergence code: 0\"\n\nif (!requireNamespace(\"MASS\", quietly = TRUE)) install.packages(\"MASS\")\nlibrary(MASS)\n\nH &lt;- mle$hessian\nvcov_matrix &lt;- tryCatch(solve(H), error = function(e) ginv(H))\nse &lt;- sqrt(diag(vcov_matrix))\n\n# Confidence intervals\nz &lt;- qnorm(0.975)\nlower &lt;- mle$par - z * se\nupper &lt;- mle$par + z * se\n\nmle_results &lt;- data.frame(\n  Parameter = c(\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"),\n  Estimate = round(mle$par, 4),\n  Std_Error = round(se, 4),\n  CI_Lower = round(lower, 4),\n  CI_Upper = round(upper, 4)\n)\n\nprint(mle_results)\n\n     Parameter Estimate Std_Error CI_Lower CI_Upper\n1 beta_netflix   0.9412    0.1110   0.7236   1.1588\n2   beta_prime   0.5016    0.1111   0.2839   0.7194\n3     beta_ads  -0.7320    0.0878  -0.9041  -0.5599\n4   beta_price  -0.0995    0.0063  -0.1119  -0.0871"
  },
  {
    "objectID": "blog/Project 3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/Project 3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nNow that we have estimated the model using maximum likelihood, we will switch to a Bayesian approach. In this part, we use a Metropolis-Hastings MCMC sampler to estimate the posterior distribution of the model parameters. This method allows us to incorporate prior beliefs and better account for uncertainty. The following instructions guide how to set up and run the MCMC algorithm step by step.\nTo begin the Bayesian estimation, I defined prior distributions for each parameter. I used Normal(0, 5) priors for the binary variables and a Normal(0, 1) prior for price. These priors reflect modest beliefs centered at zero. I then created a log-posterior function by combining the log-prior and log-likelihood.\n\nlog_prior &lt;- function(beta) {\n  dnorm(beta[1], 0, sqrt(5), log = TRUE) +  \n    dnorm(beta[2], 0, sqrt(5), log = TRUE) +  \n    dnorm(beta[3], 0, sqrt(5), log = TRUE) +  \n    dnorm(beta[4], 0, 1, log = TRUE)         \n}\n\nlog_posterior &lt;- function(beta) {\n  -log_likelihood(beta) + log_prior(beta)\n}\n\n\nWith the log-posterior function in place, I implemented a Metropolis-Hastings MCMC sampler. At each iteration, the algorithm proposes a new parameter vector using independent normal distributions and decides whether to accept it based on the log-acceptance ratio.\nNext, I ran the MCMC algorithm for 11,000 steps, discarding the first 1,000 as burn-in to allow the chain to stabilize. I then summarized the posterior draws by computing the means, standard deviations, and 95% credible intervals for each parameter.\n\n\nrun_mcmc &lt;- function(start, n_iter = 11000) {\n  draws &lt;- matrix(NA, nrow = n_iter, ncol = length(start))\n  colnames(draws) &lt;- c(\"brand_N\", \"brand_P\", \"ad_yes\", \"price\")\n  beta_current &lt;- start\n  log_post_current &lt;- log_posterior(beta_current)\n  \n  for (i in 1:n_iter) {\n    proposal &lt;- beta_current + c(\n      rnorm(1, 0, sqrt(0.05)),\n      rnorm(1, 0, sqrt(0.05)),\n      rnorm(1, 0, sqrt(0.05)),\n      rnorm(1, 0, sqrt(0.005))\n    )\n    \n    log_post_proposal &lt;- log_posterior(proposal)\n    log_accept_ratio &lt;- log_post_proposal - log_post_current\n    if (log(runif(1)) &lt; log_accept_ratio) {\n      beta_current &lt;- proposal\n      log_post_current &lt;- log_post_proposal\n    }\n    \n    draws[i, ] &lt;- beta_current\n  }\n  \n  return(draws)\n}\n\nset.seed(42)\nposterior_draws &lt;- run_mcmc(start = rep(0, 4))\n\nposterior_draws &lt;- posterior_draws[1001:11000, ]\n\npost_summary &lt;- apply(posterior_draws, 2, function(x) {\n  c(mean = mean(x), sd = sd(x), \n    lower = quantile(x, 0.025), \n    upper = quantile(x, 0.975))\n})\nround(t(post_summary), 3)\n\n          mean    sd lower.2.5% upper.97.5%\nbrand_N  0.948 0.117      0.687       1.188\nbrand_P  0.495 0.112      0.248       0.674\nad_yes  -0.732 0.096     -0.901      -0.536\nprice   -0.101 0.006     -0.113      -0.088\n\n\nTo evaluate the performance of the MCMC sampler and examine the posterior distribution, I plotted both the trace plot and the histogram for one of the parameters—brand_P. The trace plot shows the parameter values over iterations, helping to assess convergence and mixing. The histogram illustrates the shape and spread of the posterior distribution, centered around the posterior mean.\n\npar(mfrow = c(1, 2))\n\nplot(posterior_draws[, \"brand_P\"], type = \"l\",\n     main = \"Trace Plot: Beta_Price\",\n     xlab = \"Iteration\", ylab = \"Value\")\n\nhist(posterior_draws[, \"brand_P\"], breaks = 40, col = \"lightblue\", border = \"white\",\n     main = \"Posterior of Beta_Price\",\n     xlab = \"Value\", probability = TRUE)\n\nabline(v = mean(posterior_draws), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nTo evaluate the consistency and robustness of the parameter estimates, I compared the results from the Bayesian approach with those from the Maximum Likelihood Estimation (MLE) method. The table below shows the posterior means, standard deviations, and 95% credible intervals from the Bayesian method alongside the corresponding MLE estimates and confidence intervals.\n\nThis is the results fromBayesian Methods:\n\n\npost_summary\n\n              brand_N   brand_P      ad_yes        price\nmean        0.9483101 0.4945781 -0.73206852 -0.100592548\nsd          0.1171635 0.1117314  0.09583216  0.006108503\nlower.2.5%  0.6870094 0.2481402 -0.90118360 -0.112751635\nupper.97.5% 1.1882644 0.6740752 -0.53620662 -0.087806583\n\n\n\nThis is the results from Maximum Likelihood approach:\n\n\nprint(mle_results)\n\n     Parameter Estimate Std_Error CI_Lower CI_Upper\n1 beta_netflix   0.9412    0.1110   0.7236   1.1588\n2   beta_prime   0.5016    0.1111   0.2839   0.7194\n3     beta_ads  -0.7320    0.0878  -0.9041  -0.5599\n4   beta_price  -0.0995    0.0063  -0.1119  -0.0871\n\n\nComparisons:\n\nOverall Consistency: The posterior means from the Bayesian analysis are very close to the MLE point estimates, suggesting strong agreement between the two methods.\nUncertainty Quantification: The standard deviations and credible intervals from the Bayesian method are similar in width to the standard errors and confidence intervals from MLE, confirming similar levels of uncertainty.\nSlight Smoothing in Bayesian Estimates: Bayesian estimates tend to be slightly more conservative (e.g., smaller magnitude) for some parameters, especially where data may be noisier (e.g., price), due to the influence of priors.\nCredible vs. Confidence Intervals: Unlike confidence intervals from MLE, the Bayesian credible intervals can be interpreted probabilistically—for example, there is a 95% probability that the true value of beta_price lies within the credible interval.\n\nThese results indicate that the Bayesian approach provides consistent estimates with additional interpretability benefits, particularly in incorporating prior beliefs and visualizing posterior uncertainty."
  },
  {
    "objectID": "blog/Project 3/hw3_questions.html#discussion",
    "href": "blog/Project 3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nSuppose you did not simulate the data. What do you observe about the parameter estimates?\nIf we suppose the data was collected from a real-world conjoint experiment (rather than being simulated), the estimated parameters represent consumers’ revealed preferences based on their observed choices.\nThe finding that \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) indicates that, on average, consumers derive more utility from choosing a streaming service labeled as Netflix than one labeled as Amazon Prime, holding other attributes constant. This suggests that Netflix has a stronger brand appeal or is perceived as offering greater value, content, or experience compared to Prime. In the context of the multinomial logit model, it means that when two services are otherwise identical, the one branded as Netflix is more likely to be chosen than the one branded as Prime.\nIt also makes theoretical sense that \\(\\beta_\\text{price}\\) is negative. A negative coefficient on price implies that, all else equal, consumers prefer cheaper options. This reflects standard economic behavior: as the price of an alternative increases, its utility decreases, and thus the probability that it is chosen declines. This is consistent with downward-sloping demand and supports the internal validity of the model. A significant negative price coefficient also allows for computing meaningful measures such as willingness to pay (WTP) for specific features.\n\n\nWhat changes are needed to simulate and estimate a multi-level (hierarchical) model?\nTo simulate and estimate a multi-level model, you would need to:\n\nAllow each respondent to have their own set of β parameters, drawn from a common population distribution (e.g., Normal with group-level mean and variance).\nSimulate individual-level parameters (random effects) and then simulate choices based on those.\nUse methods like Hierarchical Bayes (HB) or Markov Chain Monte Carlo (MCMC) to estimate both individual-level parameters and population-level hyperparameters.\n\nThis approach captures heterogeneity in preferences, which is essential when analyzing real-world conjoint data where individuals often value attributes differently."
  },
  {
    "objectID": "blog/Project 4/hw4_questions.html",
    "href": "blog/Project 4/hw4_questions.html",
    "title": "Machine Learning",
    "section": "",
    "text": "let’s turn our attention to an unsupervised learning technique: K-means clustering. In this exercise, we’ll implement the K-means algorithm from scratch to deepen our understanding of how it iteratively partitions data into clusters. We’ll also visualize each step of the algorithm to gain an intuitive grasp of how the centroids move and how data points are reassigned during convergence. As a test case, we’ll apply our custom implementation to the Palmer Penguins dataset, focusing specifically on the bill length and flipper length variables. Finally, we’ll compare our custom results to those produced by R’s built-in kmeans() function to evaluate performance and clustering behavior.\nHere are the packages we need this time:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(class)\n\nFirst, read and clean the palmer penguins dataset:\n\ndf &lt;- read.csv(\"palmer_penguins.csv\")\nhead(df)\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen           36.7          19.3               193        3450\n5  Adelie Torgersen           39.3          20.6               190        3650\n6  Adelie Torgersen           38.9          17.8               181        3625\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4 female 2007\n5   male 2007\n6 female 2007\n\n\n\n# Select relevant columns and remove NAs\ndf_clean &lt;- df %&gt;%\n  select(bill_length_mm, flipper_length_mm) %&gt;%\n  drop_na()\n\n# Standardize the data\ndf_scaled &lt;- as.data.frame(scale(df_clean))\n\nTo deepen our understanding of how the K-means clustering algorithm operates under the hood, we’ll now implement the algorithm from scratch in R. This custom implementation will allow us to observe how the centroids evolve and how the points are re-clustered across iterations. We’ll include functionality to track and visualize the state of the algorithm at each step. The following function, my_kmeans(), takes in a dataset and performs K-means clustering using only the first two columns (e.g., bill length and flipper length), storing the clustering history for visualization purposes:\n\nmy_kmeans &lt;- function(data, K, max_iter = 100, seed = 123) {\n  set.seed(seed)\n  n &lt;- nrow(data)\n  centroids &lt;- data[sample(1:n, K), ]\n  data$cluster &lt;- NA\n  history &lt;- list()\n  \n  for (i in 1:max_iter) {\n    dist_matrix &lt;- as.matrix(dist(rbind(centroids, data[,1:2])))\n    dist_matrix &lt;- dist_matrix[1:K, (K+1):(K+n)]\n    data$cluster &lt;- apply(dist_matrix, 2, which.min)\n    \n    history[[i]] &lt;- list(iter = i, points = data, centroids = centroids)\n    \n    new_centroids &lt;- data %&gt;%\n      group_by(cluster) %&gt;%\n      summarise(across(everything(), mean), .groups = 'drop') %&gt;%\n      select(-cluster)\n    \n    if (all(abs(as.matrix(centroids) - as.matrix(new_centroids)) &lt; 1e-6)) break\n    centroids &lt;- new_centroids\n  }\n  return(list(result = data, centers = centroids, history = history))\n}\n\n\nK &lt;- 3\nres &lt;- my_kmeans(df_scaled, K)\n\nAfter running our custom K-means algorithm, we can now visualize how the clustering process unfolds over each iteration. By plotting the cluster assignments and centroids step-by-step, we gain valuable insights into how the algorithm converges. The following code generates a series of plots—one for each iteration—highlighting how points are reassigned and centroids are updated until the algorithm stabilizes:\n\nplots &lt;- lapply(seq_along(res$history), function(i) {\n  pdat &lt;- res$history[[i]]$points\n  cent &lt;- res$history[[i]]$centroids\n  ggplot(pdat, aes(x = bill_length_mm, y = flipper_length_mm, color = factor(cluster))) +\n    geom_point(size = 2) +\n    geom_point(data = cent, aes(x = bill_length_mm, y = flipper_length_mm), color = \"black\", shape = 8, size = 4) +\n    ggtitle(paste(\"Iteration\", i)) + theme_minimal()\n})\n\n\nplots\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]\n\n\n\n\n\n\n\n\n\nThe sequence of K-means clustering plots illustrates how the algorithm iteratively refines cluster assignments and centroid positions. In the early iterations, we observe noticeable shifts in both point classifications and centroid locations, indicating the model is still exploring the optimal partitioning of the data. As iterations progress, these changes diminish, and by around iteration 13, the cluster assignments become stable, and the centroids converge to consistent positions near the centers of the respective clusters. The final clustering result shows three well-separated groups, with centroids positioned in dense regions of their clusters. This suggests that the algorithm has successfully identified meaningful structure in the data, with good cohesion within clusters and clear separation between them.\n\nset.seed(123)\nkmeans_base &lt;- kmeans(df_scaled, centers = 3, nstart = 10)\ndf_scaled$base_kmeans &lt;- kmeans_base$cluster\n\np1 &lt;- ggplot(res$result, aes(x = bill_length_mm, y = flipper_length_mm, color = factor(cluster))) +\n  geom_point(size = 2) +\n  ggtitle(\"Custom K-means\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df_scaled, aes(x = bill_length_mm, y = flipper_length_mm, color = factor(base_kmeans))) +\n  geom_point(size = 2) +\n  ggtitle(\"Built-in kmeans()\") +\n  theme_minimal()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nThe side-by-side comparison of the clustering results from the custom K-means implementation and R’s built-in kmeans() function reveals both alignment and subtle differences. Both approaches successfully partition the data into three distinct clusters based on bill length and flipper length, indicating that the underlying structure of the data has been effectively captured.\nTo quantitatively assess the quality of clustering and determine the most appropriate number of clusters, we now turn to two widely used evaluation metrics: within-cluster sum of squares (WCSS) and the silhouette score. These metrics provide complementary insights—WCSS captures how compact the clusters are, while the silhouette score evaluates how well each point fits within its assigned cluster compared to others. In the next step, we’ll compute and plot these metrics for a range of cluster counts (K = 2 to 7) to help identify the optimal number of clusters suggested by the data.\n\nlibrary(cluster)\n\n\n# Evaluate clustering metrics for K = 2 to 7\nk_values &lt;- 2:7\nwcss &lt;- numeric(length(k_values))\nsilhouette_scores &lt;- numeric(length(k_values))\n\nfor (i in seq_along(k_values)) {\n  k &lt;- k_values[i]\n  kmeans_res &lt;- kmeans(df_scaled, centers = k, nstart = 10)\n  wcss[i] &lt;- kmeans_res$tot.withinss\n\n  sil &lt;- silhouette(kmeans_res$cluster, dist(df_scaled))\n  silhouette_scores[i] &lt;- mean(sil[, 3])\n}\n\n\nmetrics_df &lt;- data.frame(\n  K = k_values,\n  WCSS = wcss,\n  Silhouette = silhouette_scores\n)\n\n\np_wcss &lt;- ggplot(metrics_df, aes(x = K, y = WCSS)) +\n  geom_line() + geom_point() +\n  ggtitle(\"Within-Cluster Sum of Squares (WCSS)\") +\n  xlab(\"Number of Clusters (K)\") + ylab(\"WCSS\") +\n  theme_minimal()\n\np_sil &lt;- ggplot(metrics_df, aes(x = K, y = Silhouette)) +\n  geom_line() + geom_point() +\n  ggtitle(\"Average Silhouette Score\") +\n  xlab(\"Number of Clusters (K)\") + ylab(\"Silhouette Score\") +\n  theme_minimal()\n\n\np_wcss\n\n\n\n\n\n\n\n\n\np_sil\n\n\n\n\n\n\n\n\n\nbest_k_wcss &lt;- which.min(diff(diff(wcss))) + 2\nbest_k_sil &lt;- k_values[which.max(silhouette_scores)]\ncat(\"Suggested number of clusters based on WCSS elbow method:\", best_k_wcss, \"\\n\")\n\nSuggested number of clusters based on WCSS elbow method: 6 \n\ncat(\"Suggested number of clusters based on silhouette score:\", best_k_sil, \"\\n\")\n\nSuggested number of clusters based on silhouette score: 3 \n\n\nThe comparison of clustering quality using both the within-cluster sum of squares (WCSS) and the silhouette score provides valuable guidance in selecting an appropriate number of clusters. According to the elbow method applied to WCSS, the optimal number of clusters appears to be 6, where the rate of decrease in WCSS begins to level off. In contrast, the silhouette score, which evaluates both cohesion and separation, peaks at K = 3, suggesting that three clusters provide the best-defined groupings in terms of structure and separation.\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "blog/Project 4/hw4_questions.html#a.-k-means",
    "href": "blog/Project 4/hw4_questions.html#a.-k-means",
    "title": "Machine Learning",
    "section": "",
    "text": "let’s turn our attention to an unsupervised learning technique: K-means clustering. In this exercise, we’ll implement the K-means algorithm from scratch to deepen our understanding of how it iteratively partitions data into clusters. We’ll also visualize each step of the algorithm to gain an intuitive grasp of how the centroids move and how data points are reassigned during convergence. As a test case, we’ll apply our custom implementation to the Palmer Penguins dataset, focusing specifically on the bill length and flipper length variables. Finally, we’ll compare our custom results to those produced by R’s built-in kmeans() function to evaluate performance and clustering behavior.\nHere are the packages we need this time:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(class)\n\nFirst, read and clean the palmer penguins dataset:\n\ndf &lt;- read.csv(\"palmer_penguins.csv\")\nhead(df)\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen           36.7          19.3               193        3450\n5  Adelie Torgersen           39.3          20.6               190        3650\n6  Adelie Torgersen           38.9          17.8               181        3625\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4 female 2007\n5   male 2007\n6 female 2007\n\n\n\n# Select relevant columns and remove NAs\ndf_clean &lt;- df %&gt;%\n  select(bill_length_mm, flipper_length_mm) %&gt;%\n  drop_na()\n\n# Standardize the data\ndf_scaled &lt;- as.data.frame(scale(df_clean))\n\nTo deepen our understanding of how the K-means clustering algorithm operates under the hood, we’ll now implement the algorithm from scratch in R. This custom implementation will allow us to observe how the centroids evolve and how the points are re-clustered across iterations. We’ll include functionality to track and visualize the state of the algorithm at each step. The following function, my_kmeans(), takes in a dataset and performs K-means clustering using only the first two columns (e.g., bill length and flipper length), storing the clustering history for visualization purposes:\n\nmy_kmeans &lt;- function(data, K, max_iter = 100, seed = 123) {\n  set.seed(seed)\n  n &lt;- nrow(data)\n  centroids &lt;- data[sample(1:n, K), ]\n  data$cluster &lt;- NA\n  history &lt;- list()\n  \n  for (i in 1:max_iter) {\n    dist_matrix &lt;- as.matrix(dist(rbind(centroids, data[,1:2])))\n    dist_matrix &lt;- dist_matrix[1:K, (K+1):(K+n)]\n    data$cluster &lt;- apply(dist_matrix, 2, which.min)\n    \n    history[[i]] &lt;- list(iter = i, points = data, centroids = centroids)\n    \n    new_centroids &lt;- data %&gt;%\n      group_by(cluster) %&gt;%\n      summarise(across(everything(), mean), .groups = 'drop') %&gt;%\n      select(-cluster)\n    \n    if (all(abs(as.matrix(centroids) - as.matrix(new_centroids)) &lt; 1e-6)) break\n    centroids &lt;- new_centroids\n  }\n  return(list(result = data, centers = centroids, history = history))\n}\n\n\nK &lt;- 3\nres &lt;- my_kmeans(df_scaled, K)\n\nAfter running our custom K-means algorithm, we can now visualize how the clustering process unfolds over each iteration. By plotting the cluster assignments and centroids step-by-step, we gain valuable insights into how the algorithm converges. The following code generates a series of plots—one for each iteration—highlighting how points are reassigned and centroids are updated until the algorithm stabilizes:\n\nplots &lt;- lapply(seq_along(res$history), function(i) {\n  pdat &lt;- res$history[[i]]$points\n  cent &lt;- res$history[[i]]$centroids\n  ggplot(pdat, aes(x = bill_length_mm, y = flipper_length_mm, color = factor(cluster))) +\n    geom_point(size = 2) +\n    geom_point(data = cent, aes(x = bill_length_mm, y = flipper_length_mm), color = \"black\", shape = 8, size = 4) +\n    ggtitle(paste(\"Iteration\", i)) + theme_minimal()\n})\n\n\nplots\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]\n\n\n\n\n\n\n\n\n\nThe sequence of K-means clustering plots illustrates how the algorithm iteratively refines cluster assignments and centroid positions. In the early iterations, we observe noticeable shifts in both point classifications and centroid locations, indicating the model is still exploring the optimal partitioning of the data. As iterations progress, these changes diminish, and by around iteration 13, the cluster assignments become stable, and the centroids converge to consistent positions near the centers of the respective clusters. The final clustering result shows three well-separated groups, with centroids positioned in dense regions of their clusters. This suggests that the algorithm has successfully identified meaningful structure in the data, with good cohesion within clusters and clear separation between them.\n\nset.seed(123)\nkmeans_base &lt;- kmeans(df_scaled, centers = 3, nstart = 10)\ndf_scaled$base_kmeans &lt;- kmeans_base$cluster\n\np1 &lt;- ggplot(res$result, aes(x = bill_length_mm, y = flipper_length_mm, color = factor(cluster))) +\n  geom_point(size = 2) +\n  ggtitle(\"Custom K-means\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df_scaled, aes(x = bill_length_mm, y = flipper_length_mm, color = factor(base_kmeans))) +\n  geom_point(size = 2) +\n  ggtitle(\"Built-in kmeans()\") +\n  theme_minimal()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nThe side-by-side comparison of the clustering results from the custom K-means implementation and R’s built-in kmeans() function reveals both alignment and subtle differences. Both approaches successfully partition the data into three distinct clusters based on bill length and flipper length, indicating that the underlying structure of the data has been effectively captured.\nTo quantitatively assess the quality of clustering and determine the most appropriate number of clusters, we now turn to two widely used evaluation metrics: within-cluster sum of squares (WCSS) and the silhouette score. These metrics provide complementary insights—WCSS captures how compact the clusters are, while the silhouette score evaluates how well each point fits within its assigned cluster compared to others. In the next step, we’ll compute and plot these metrics for a range of cluster counts (K = 2 to 7) to help identify the optimal number of clusters suggested by the data.\n\nlibrary(cluster)\n\n\n# Evaluate clustering metrics for K = 2 to 7\nk_values &lt;- 2:7\nwcss &lt;- numeric(length(k_values))\nsilhouette_scores &lt;- numeric(length(k_values))\n\nfor (i in seq_along(k_values)) {\n  k &lt;- k_values[i]\n  kmeans_res &lt;- kmeans(df_scaled, centers = k, nstart = 10)\n  wcss[i] &lt;- kmeans_res$tot.withinss\n\n  sil &lt;- silhouette(kmeans_res$cluster, dist(df_scaled))\n  silhouette_scores[i] &lt;- mean(sil[, 3])\n}\n\n\nmetrics_df &lt;- data.frame(\n  K = k_values,\n  WCSS = wcss,\n  Silhouette = silhouette_scores\n)\n\n\np_wcss &lt;- ggplot(metrics_df, aes(x = K, y = WCSS)) +\n  geom_line() + geom_point() +\n  ggtitle(\"Within-Cluster Sum of Squares (WCSS)\") +\n  xlab(\"Number of Clusters (K)\") + ylab(\"WCSS\") +\n  theme_minimal()\n\np_sil &lt;- ggplot(metrics_df, aes(x = K, y = Silhouette)) +\n  geom_line() + geom_point() +\n  ggtitle(\"Average Silhouette Score\") +\n  xlab(\"Number of Clusters (K)\") + ylab(\"Silhouette Score\") +\n  theme_minimal()\n\n\np_wcss\n\n\n\n\n\n\n\n\n\np_sil\n\n\n\n\n\n\n\n\n\nbest_k_wcss &lt;- which.min(diff(diff(wcss))) + 2\nbest_k_sil &lt;- k_values[which.max(silhouette_scores)]\ncat(\"Suggested number of clusters based on WCSS elbow method:\", best_k_wcss, \"\\n\")\n\nSuggested number of clusters based on WCSS elbow method: 6 \n\ncat(\"Suggested number of clusters based on silhouette score:\", best_k_sil, \"\\n\")\n\nSuggested number of clusters based on silhouette score: 3 \n\n\nThe comparison of clustering quality using both the within-cluster sum of squares (WCSS) and the silhouette score provides valuable guidance in selecting an appropriate number of clusters. According to the elbow method applied to WCSS, the optimal number of clusters appears to be 6, where the rate of decrease in WCSS begins to level off. In contrast, the silhouette score, which evaluates both cohesion and separation, peaks at K = 3, suggesting that three clusters provide the best-defined groupings in terms of structure and separation.\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "blog/Project 4/hw4_questions.html#b.-latent-class-mnl",
    "href": "blog/Project 4/hw4_questions.html#b.-latent-class-mnl",
    "title": "Machine Learning",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "blog/Project 4/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "blog/Project 4/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Machine Learning",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\nIn this step, we generate a synthetic dataset specifically designed to test the performance of K-Nearest Neighbors (KNN) classifiers. The dataset consists of two features, x1 and x2, both uniformly sampled from a range of -3 to 3. This synthetic setting allows us to later evaluate how well KNN adapts to nonlinear decision boundaries, making it an excellent test case for comparing custom and built-in implementations of KNN.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\nOnce the synthetic dataset has been generated, the next step is to visualize the data to understand how the binary outcome variable y relates to the two features x1 and x2. In this plot, the horizontal axis represents x1, and the vertical axis represents x2. Each point is colored according to the value of y, indicating its class label.\nThis visualization provides a clear view of the classification structure, showing how the outcome is determined by a nonlinear, wiggly boundary defined by the function sin(4 * x1) + x1. Optionally, this decision boundary can be drawn on the plot as a curve to illustrate how the classes are separated. This step is crucial for developing an intuition about the complexity of the classification task and how well KNN might perform in this scenario.\n\nggplot(dat, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 2) +\n  stat_function(fun = function(x) sin(4*x) + x, color = \"black\", linetype = \"dashed\") +\n  labs(title = \"Synthetic Data with Wiggly Boundary\",\n       x = \"x1\",\n       y = \"x2\",\n       color = \"Class\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot shows a synthetic dataset where each point is classified based on whether it lies above or below a wiggly decision boundary. Points are colored by class: class 1 (cyan) lies above the boundary, and class 0 (red) lies below.\nThis setup creates a non-linear separation between classes, making it ideal for evaluating flexible models like K-Nearest Neighbors (KNN). The clear division, with some overlap near the boundary, highlights the challenge for classifiers and the importance of tuning K to capture local patterns without overfitting.\nAnd then we create the test dataset, using the same way above:\n\nset.seed(123)\nx1_test &lt;- runif(n, -3, 3)\nx2_test &lt;- runif(n, -3, 3)\nx_test &lt;- cbind(x1_test, x2_test)\nboundary_test &lt;- sin(4*x1_test) + x1_test\ny_test &lt;- ifelse(x2_test &gt; boundary_test, 1, 0) |&gt; as.factor()\ndat_test &lt;- data.frame(x1 = x1_test, x2 = x2_test, y = y_test)\n\nWith the synthetic dataset visualized and understood, the next step is to implement the K-Nearest Neighbors (KNN) algorithm from scratch. This involves classifying test points by finding the k closest training points using Euclidean distance and assigning the most common label among them. By writing this manually, we gain a clearer understanding of how KNN operates at the algorithmic level. To validate our custom implementation, we also compare the results with R’s built-in class::knn() function, which provides a benchmark for correctness and performance.\n\nknn_predict &lt;- function(train_X, train_y, test_X, k = 5) {\n  n_test &lt;- nrow(test_X)\n  preds &lt;- character(n_test)\n\n  euclidean &lt;- function(a, b) {\n    sqrt(sum((a - b)^2))\n  }\n\n  for (i in 1:n_test) {\n    dists &lt;- apply(train_X, 1, function(row) euclidean(row, test_X[i, ]))\n    nearest_indices &lt;- order(dists)[1:k]\n    nearest_labels &lt;- train_y[nearest_indices]\n    preds[i] &lt;- names(which.max(table(nearest_labels)))\n  }\n\n  return(factor(preds, levels = levels(train_y)))\n}\n\n\ntrain_X &lt;- dat[, c(\"x1\", \"x2\")]\ntrain_y &lt;- dat$y\ntest_X &lt;- dat_test[, c(\"x1\", \"x2\")]\ntest_y &lt;- dat_test$y\n\npred_manual &lt;- knn_predict(train_X, train_y, test_X, k = 5)\n\n# Accuracy of manual implementation\ncat(\"Accuracy (manual KNN):\", mean(pred_manual == test_y), \"\\n\")\n\nAccuracy (manual KNN): 0.9 \n\n# Compare with built-in KNN\npred_builtin &lt;- knn(train = train_X, test = test_X, cl = train_y, k = 5)\ncat(\"Accuracy (class::knn):\", mean(pred_builtin == test_y), \"\\n\")\n\nAccuracy (class::knn): 0.9 \n\n\nAfter running both the manual and built-in KNN implementations, we observe that both achieved an accuracy of 0.9 on the test dataset. This strong agreement confirms that the manual algorithm is correctly implemented. The high accuracy also suggests that the chosen value of k (in this case, k = 5) is effective at capturing the local decision structure in the data without overfitting. This step demonstrates that KNN is a robust choice for handling nonlinear boundaries when tuned properly and highlights the value of verifying custom code against reliable library functions.\nTo evaluate the performance of our K-Nearest Neighbors (KNN) classifier more thoroughly, we now explore how classification accuracy changes with different values of k, the number of neighbors considered. Specifically, we run our KNN function for each value of k from 1 to 30, recording the proportion of correctly classified points on the test dataset for each case.\nThis process allows us to generate a plot showing how accuracy varies as k increases. By visualizing this relationship, we can identify the optimal value of k.\n\n# Evaluate accuracy for k = 1 to 30\nk_values &lt;- 1:30\naccuracy_scores &lt;- numeric(length(k_values))\n\nfor (k in k_values) {\n  preds &lt;- knn_predict(train_X, train_y, test_X, k)\n  accuracy_scores[k] &lt;- mean(preds == test_y)\n}\n\n# Plot accuracy vs. k\naccuracy_df &lt;- data.frame(k = k_values, accuracy = accuracy_scores)\n\nggplot(accuracy_df, aes(x = k, y = accuracy)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"steelblue\") +\n  labs(title = \"KNN Accuracy vs. K\",\n       x = \"Number of Neighbors (K)\",\n       y = \"Accuracy\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nbest_k &lt;- k_values[which.max(accuracy_scores)]\nbest_accuracy &lt;- max(accuracy_scores)\ncat(\"\\nOptimal k:\", best_k, \"with accuracy:\", round(best_accuracy, 4), \"\\n\")\n\n\nOptimal k: 15 with accuracy: 0.98 \n\n\nAfter running the K-Nearest Neighbors (KNN) classifier for values of k from 1 to 30, we find that the optimal value is k = 15, which achieved the highest accuracy of 0.98 on the test dataset. This indicates that using 15 neighbors for classification strikes a strong balance between bias and variance: it smooths out local noise while still capturing the non-linear structure of the data. The high accuracy at k=15 suggests that the classifier can generalize well to new, unseen data under this setting — confirming that KNN, when carefully tuned, can perform very effectively on problems with non-linear decision boundaries."
  },
  {
    "objectID": "blog/Project 4/hw4_questions.html#b.-key-drivers-analysis",
    "href": "blog/Project 4/hw4_questions.html#b.-key-drivers-analysis",
    "title": "Machine Learning",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  }
]