labs(title = "Response Rate by Treatment Group",
x = "Group",
y = "Proportion Donated") +
scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
theme_minimal() +
theme(legend.position = "none")
# Create summary stats by treatment
df %>%
mutate(group = ifelse(treatment == 1, "Treatment", "Control")) %>%
group_by(group) %>%
summarise(
response_rate = round(mean(gave), 3),
unconditional_donation = round(mean(amount), 3),
conditional_donation = round(mean(amount[gave == 1]), 3),
observations  = n()
)
# Manual t-test for difference in response rates
x_treat <- mean(df$gave[df$treatment == 1])
x_ctrl <- mean(df$gave[df$treatment == 0])
s2_treat <- var(df$gave[df$treatment == 1])
s2_ctrl <- var(df$gave[df$treatment == 0])
n_treat <- sum(df$treatment == 1)
n_ctrl <- sum(df$treatment == 0)
se_diff <- sqrt(s2_treat / n_treat + s2_ctrl / n_ctrl)
t_stat <- (x_treat - x_ctrl) / se_diff
se_diff
t_stat
model <- lm(gave ~ treatment, data = df)
summary(model)
library(margins)
# Probit regression: same as Table 3, Column 1
probit_model <- glm(gave ~ treatment, data = df, family = binomial(link = "probit"))
# Show regression output
summary(probit_model)
# Calculate marginal effect of treatment
mfx <- margins(probit_model)
summary(mfx)
# Filter for treatment groups only (exclude control)
df_treat <- df %>% filter(treatment == 1)
# Make sure ratio is a factor for grouping
df_treat <- df_treat %>% mutate(ratio = as.character(ratio))
# 1. Compare 2:1 vs. 1:1
gave_2_1 <- df_treat$gave[df_treat$ratio == "2"]
gave_1_1 <- df_treat$gave[df_treat$ratio == "1"]
t_2_vs_1 <- t.test(gave_2_1, gave_1_1, var.equal = TRUE)
print(t_2_vs_1)
# 2. Compare 3:1 vs. 1:1
gave_3_1 <- df_treat$gave[df_treat$ratio == "3"]
t_3_vs_1 <- t.test(gave_3_1, gave_1_1, var.equal = TRUE)
print(t_3_vs_1)
# 3. Compare 3:1 vs. 2:1
t_3_vs_2 <- t.test(gave_3_1, gave_2_1, var.equal = TRUE)
print(t_3_vs_2)
model_ratio <- lm(gave ~ ratio, data = df %>% filter(treatment == 1))
summary(model_ratio)
library(tidyr)
# Calculate raw response rates for each match ratio
df %>%
filter(ratio %in% c("1", "2", "3")) %>%
group_by(ratio) %>%
summarise(response_rate = mean(gave, na.rm = TRUE)) %>%
pivot_wider(names_from = ratio, values_from = response_rate) -> rate_table
# Calculate differences
diff_2_vs_1 <- rate_table$`2` - rate_table$`1`
diff_3_vs_2 <- rate_table$`3` - rate_table$`2`
cat("Response rate difference (2:1 vs 1:1):", round(diff_2_vs_1, 4), "\n")
cat("Response rate difference (3:1 vs 2:1):", round(diff_3_vs_2, 4), "\n")
coef_table <- summary(model_ratio)$coefficients
# Extract coefficients
coef_1_1 <- coef_table["(Intercept)", "Estimate"]
coef_2_1 <- coef_table["ratio2", "Estimate"]
model_ratio <- lm(gave ~ factor(ratio), data = df %>% filter(treatment == 1))
summary(model_ratio)
library(tidyr)
# Calculate raw response rates for each match ratio
df %>%
filter(ratio %in% c("1", "2", "3")) %>%
group_by(ratio) %>%
summarise(response_rate = mean(gave, na.rm = TRUE)) %>%
pivot_wider(names_from = ratio, values_from = response_rate) -> rate_table
# Calculate differences
diff_2_vs_1 <- rate_table$`2` - rate_table$`1`
diff_3_vs_2 <- rate_table$`3` - rate_table$`2`
cat("Response rate difference (2:1 vs 1:1):", round(diff_2_vs_1, 4), "\n")
cat("Response rate difference (3:1 vs 2:1):", round(diff_3_vs_2, 4), "\n")
coef_table <- summary(model_ratio)$coefficients
# Extract coefficients
coef_1_1 <- coef_table["(Intercept)", "Estimate"]
coef_2_1 <- coef_table["ratio2", "Estimate"]
coef_table <- summary(model_ratio)$coefficients
# Extract coefficients
coef_1_1 <- coef_table["(Intercept)", "Estimate"]
coef_2_1 <- coef_table["factor(ratio)2", "Estimate"]
coef_3_1 <- coef_table["factor(ratio)3", "Estimate"]
# Differences between fitted values
fitted_2_vs_1 <- coef_2_1
fitted_3_vs_2 <- coef_3_1 - coef_2_1
cat("Fitted response rate difference (2:1 vs 1:1):", round(fitted_2_vs_1, 4), "\n")
cat("Fitted response rate difference (3:1 vs 2:1):", round(fitted_3_vs_2, 4), "\n")
data <- read.csv("airbnb.csv")
data <- read.csv("airbnb.csv")
data
df <- read.csv("blueprinty.csv")
df
library(ggplot2)
ggplot(data, aes(x = patents, fill = factor(iscustomer))) +
geom_histogram(position = "dodge", bins = 30) +
labs(title = "Histogram of Patents by Customer Status",
x = "Number of Patents", fill = "Customer") +
theme_minimal()
library(ggplot2)
ggplot(data, aes(x = patents, fill = factor(iscustomer))) +
geom_histogram(bins = 30) +
labs(title = "Histogram of Patents by Customer Status",
x = "Number of Patents", fill = "Customer") +
theme_minimal()
library(ggplot2)
ggplot(df, aes(x = patents, fill = factor(iscustomer))) +
geom_histogram(position = "dodge",bins = 30) +
labs(title = "Histogram of Patents by Customer Status",
x = "Number of Patents", fill = "Customer") +
theme_minimal()
aggregate(patents ~ iscustomer, data = df, mean)
ggplot(data, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +
geom_boxplot() +
labs(title = "Firm Age by Customer Status",
x = "Customer (0 = No, 1 = Yes)", y = "Age") +
theme_minimal()
ggplot(df, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +
geom_boxplot() +
labs(title = "Firm Age by Customer Status",
x = "Customer (0 = No, 1 = Yes)", y = "Age") +
theme_minimal()
aggregate(age ~ iscustomer, data = df, summary)
aggregate(age ~ iscustomer, data = df, summary)
ggplot(data, aes(x = region, fill = factor(iscustomer))) +
geom_bar(position = "fill") +
labs(title = "Region Distribution by Customer Status",
x = "Region", y = "Proportion", fill = "Customer") +
theme_minimal()
ggplot(df, aes(x = region, fill = factor(iscustomer))) +
geom_bar(position = "fill") +
labs(title = "Region Distribution by Customer Status",
x = "Region", y = "Proportion", fill = "Customer") +
theme_minimal()
prop.table(table(data$region, data$iscustomer), 1)
prop.table(table(df$region, df$iscustomer), 1)
aggregate(age ~ iscustomer, data = df, summary)
aggregate(age ~ iscustomer, data = df, mean)
$Y \sim \text{Poisson}(\lambda)$. Note that $f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y!$
poisson_loglikelihood <- function(lambda, Y){
if (lambda <= 0) {
return(-Inf)  # log-likelihood is undefined for non-positive lambda
}
n <- length(Y)
log_likelihood <- -n * lambda + sum(Y) * log(lambda) - sum(lgamma(Y + 1))
return(log_likelihood)
}
Y <- df$patents
lambda_values <- seq(0.1, 10, length.out = 200)
log_likelihoods <- sapply(lambda_values, poisson_loglikelihood, Y = Y)
plot(lambda_values, log_likelihoods, type = "l",
xlab = "Lambda", ylab = "Log-Likelihood",
main = "Poisson Log-Likelihood as a Function of Lambda")
neg_loglikelihood <- function(lambda, Y) {
if (lambda <= 0) return(Inf)
n <- length(Y)
-(-n * lambda + sum(Y) * log(lambda) - sum(lgamma(Y + 1)))
}
# Use optim to minimize the negative log-likelihood
result <- optim(par = 1, fn = neg_loglikelihood, Y = Y, method = "Brent", lower = 0.001, upper = 20)
# Print the MLE
lambda_mle <- result$par
cat("MLE for lambda:", lambda_mle, "\n")
poisson_regression_loglikelihood <- function(beta, Y, X) {
eta <- X %*% beta                     # Linear predictor: X * beta
lambda <- exp(eta)                   # Inverse link: lambda = exp(X * beta)
# Log-likelihood for Poisson: sum(Y * log(lambda) - lambda - log(Y!))
log_lik <- sum(Y * log(lambda) - lambda - lgamma(Y + 1))
return(-log_lik)  # Negative log-likelihood for use in optim()
}
df$age2 <- df$age^2
df$region <- factor(df$region)
X <- model.matrix(~ age + age2 + region + iscustomer, data = df)  # automatically adds intercept
init_beta <- rep(0, ncol(X))  # initial guess
fit <- optim(par = init_beta,
fn = poisson_regression_loglikelihood,
Y = Y, X = X,
method = "BFGS", hessian = TRUE)
beta_hat <- fit$par
hessian <- fit$hessian
var_cov_matrix <- solve(hessian)  # inverse of Hessian = variance-covariance matrix
std_errors <- sqrt(diag(var_cov_matrix))
# Create result table
results <- data.frame(
Coefficient = beta_hat,
Std_Error = std_errors,
row.names = colnames(X)
)
print(round(results, 4))
poisson_model <- glm(patents ~ age + age2 + region + iscustomer,
family = poisson(link = "log"),
data = data)
poisson_model <- glm(patents ~ age + age2 + region + iscustomer,
family = poisson(link = "log"),
data = df)
# Display regression summary
summary(poisson_model)
# Fit the model using glm
model <- glm(patents ~ age + I(age^2) + region + iscustomer,
family = poisson(link = "log"), data = df)
# Create two copies of the data
data_0 <- df
data_1 <- df
# Set iscustomer = 0 and 1 respectively
data_0$iscustomer <- 0
data_1$iscustomer <- 1
# Predict expected number of patents under each scenario
y_pred_0 <- predict(model, newdata = data_0, type = "response")
y_pred_1 <- predict(model, newdata = data_1, type = "response")
# Compute the difference and take the average
effect <- mean(y_pred_1 - y_pred_0)
cat("Average estimated effect of being a customer on patents:", round(effect, 4), "\n")
data_clean <- na.omit(data[, c("number_of_reviews", "days", "room_type", "bathrooms", "bedrooms", "price", "review_scores_cleanliness", "review_scores_location", "review_scores_value", "instant_bookable")])
data <- read.csv("airbnb.csv")
data
data_clean <- na.omit(data[, c("number_of_reviews", "days", "room_type", "bathrooms", "bedrooms", "price", "review_scores_cleanliness", "review_scores_location", "review_scores_value", "instant_bookable")])
data_clean <- na.omit(data[, c("number_of_reviews", "days", "room_type", "bathrooms", "bedrooms", "price", "review_scores_cleanliness", "review_scores_location", "review_scores_value", "instant_bookable")])
df_clean
data_clean
model <- glm(number_of_reviews ~ days + room_type + bathrooms + bedrooms + price +
review_scores_cleanliness + review_scores_location + review_scores_value +
instant_bookable,
family = poisson(link = "log"), data = df_clean)
model <- glm(number_of_reviews ~ days + room_type + bathrooms + bedrooms + price +
review_scores_cleanliness + review_scores_location + review_scores_value +
instant_bookable,
family = poisson(link = "log"), data = data_clean)
summary(model)
df_clean$instant_bookable <- ifelse(df_clean$instant_bookable == "t", 1, 0)
data_clean$instant_bookable <- ifelse(data_clean$instant_bookable == "t", 1, 0)
data_clean
model <- glm(number_of_reviews ~ days + room_type + bathrooms + bedrooms + price +
review_scores_cleanliness + review_scores_location + review_scores_value +
instant_bookable,
family = poisson(link = "log"), data = data_clean)
summary(model)
df <- read.csv("blueprinty.csv")
df.head()
df <- read.csv("blueprinty.csv")
head(df)
aggregate(patents ~ iscustomer, data = df, mean)
prop.table(table(df$region, df$iscustomer), 1)
aggregate(age ~ iscustomer, data = df, mean)
ggplot(df, aes(x = region, fill = factor(iscustomer))) +
geom_bar(position = "fill") +
labs(title = "Region Distribution by Customer Status",
x = "Region", y = "Proportion", fill = "Customer") +
theme_minimal()
ggplot(df, aes(x = factor(iscustomer), y = age, fill = factor(iscustomer))) +
geom_boxplot() +
labs(title = "Firm Age by Customer Status",
x = "Customer (0 = No, 1 = Yes)", y = "Age") +
theme_minimal()
poisson_likelihood <- function(lambda, Y) {
likelihoods <- (exp(-lambda) * lambda^Y) / factorial(Y)
total_likelihood <- prod(likelihoods)
return(total_likelihood)
}
library(ggplot2)
ggplot(df, aes(x = patents, fill = factor(iscustomer))) +
geom_histogram(position = "dodge",bins = 30) +
labs(title = "Histogram of Patents by Customer Status",
x = "Number of Patents", fill = "Customer") +
theme_minimal()
$$
$$
data <- read.csv("airbnb.csv")
head(data)
data_clean <- na.omit(data[, c("number_of_reviews", "days", "room_type", "bathrooms", "bedrooms", "price", "review_scores_cleanliness", "review_scores_location", "review_scores_value", "instant_bookable")])
data_clean$instant_bookable <- ifelse(data_clean$instant_bookable == "t", 1, 0)
head(data_clean)
head(data_clean，10)
head(data_clean,10)
head(data_clean,20)
model <- glm(number_of_reviews ~ days + room_type + bathrooms + bedrooms + price +
review_scores_cleanliness + review_scores_location + review_scores_value +
instant_bookable,
family = poisson(link = "log"), data = data_clean)
summary(model)
# Load data
conjoint_data <- read.csv("conjoint_data.csv")
# Load data
conjoint_data <- read.csv("conjoint_data.csv")
# Load data
conjoint_data <- read.csv("conjoint_data.csv")
head(conjoint_data)
# Convert brand and ad to dummy variables
conjoint_data$brand_N <- as.integer(conjoint_data$brand == "N")
conjoint_data$brand_P <- as.integer(conjoint_data$brand == "P")
conjoint_data$ad_Yes  <- as.integer(conjoint_data$ad == "Yes")
# OPTIONAL: Sort by respondent and task to keep choice sets grouped
conjoint_data <- conjoint_data[order(conjoint_data$resp, conjoint_data$task), ]
# Create a unique task ID (e.g., "1_1", "1_2", ...)
conjoint_data$task_id <- paste(conjoint_data$resp, conjoint_data$task, sep = "_")
# Create design matrix (X) and choice vector (y)
X <- as.matrix(conjoint_data[, c("brand_N", "brand_P", "ad_Yes", "price")])
y <- conjoint_data$choice
# Preview final data structure
head(cbind(conjoint_data$task_id, y, X))
# Create dummy variables
conjoint_data <- conjoint_data %>%
mutate(
brand_N = as.integer(brand == "N"),
brand_P = as.integer(brand == "P"),
ad_Yes  = as.integer(ad == "Yes")
)
# Load data
conjoint_data <- read.csv("conjoint_data.csv")
head(conjoint_data)
# Create dummy variables
conjoint_data <- conjoint_data %>%
mutate(
brand_N = as.integer(brand == "N"),
brand_P = as.integer(brand == "P"),
ad_Yes  = as.integer(ad == "Yes")
)
library(dplyr)
library(tidyr)
# Create dummy variables
conjoint_data <- conjoint_data %>%
mutate(
brand_N = as.integer(brand == "N"),
brand_P = as.integer(brand == "P"),
ad_Yes  = as.integer(ad == "Yes")
)
# Sort data
conjoint_data <- conjoint_data %>%
arrange(resp, task)
conjoint_data
# Define log-likelihood function
log_likelihood <- function(beta, data) {
X <- as.matrix(data[, c("brand_N", "brand_P", "ad_Yes", "price")])
y <- data$choice
n <- nrow(data)
# Linear index
Xb <- X %*% beta
# Grouping: 3 alternatives per task
groups <- rep(1:(n / 3), each = 3)
# Compute log-likelihood
log_lik <- tapply(1:n, groups, function(idx) {
xb <- Xb[idx]
chosen <- y[idx]
log(sum_exp <- sum(exp(xb))) - xb[which(chosen == 1)]
})
return(-sum(unlist(log_lik)))  # Negative log-likelihood for minimization
}
# Initial values
init_beta <- c(0, 0, 0, 0)  # beta_N, beta_P, beta_ads, beta_price
# Run optimization
mle <- optim(
par = init_beta,
fn = log_likelihood,
data = conjoint_data,
method = "BFGS",
hessian = TRUE
)
log_likelihood <- function(beta, data) {
X <- as.matrix(data[, c("brand_N", "brand_P", "ad_Yes", "price")])
y <- data$choice
n <- nrow(data)
Xb <- X %*% beta
groups <- rep(1:(n / 3), each = 3)
log_lik <- tryCatch({
tapply(1:n, groups, function(idx) {
xb <- Xb[idx]
chosen <- y[idx]
denom <- log(sum(exp(xb)))
chosen_utility <- xb[which(chosen == 1)]
if (length(chosen_utility) == 0 || is.infinite(denom)) return(NA)
return(chosen_utility - denom)
}) |> unlist() |> sum(na.rm = TRUE)
}, error = function(e) return(NA))
# Return NEGATIVE log-likelihood for minimization
if (is.na(log_lik) || is.infinite(log_lik)) return(1e6)  # return large value to penalize
return(-log_lik)
}
# Initial values
init_beta <- c(0, 0, 0, 0)  # beta_N, beta_P, beta_ads, beta_price
# Run optimization
mle <- optim(
par = init_beta,
fn = log_likelihood,
data = conjoint_data,
method = "BFGS",
hessian = TRUE
)
# Estimated coefficients
mle$par
# Extract Hessian
H <- mle$hessian
# Compute variance-covariance matrix
vcov_matrix <- solve(H)
mle$convergence  # should be 0 if converged
H <- tryCatch({
mle$hessian
}, error = function(e) {
hessian(func = log_likelihood, x = mle$par, data = conjoint_data)
})
vcov_matrix <- tryCatch({
solve(H)
}, error = function(e) {
ginv(H)
})
library(dplyr)
library(tidyr)
library(MASS)
library(numDeriv)
install.packages("numDeriv")
library(dplyr)
library(tidyr)
library(MASS)
library(numDeriv)
vcov_matrix <- tryCatch({
solve(H)
}, error = function(e) {
ginv(H)
})
se <- sqrt(diag(vcov_matrix))
z <- qnorm(0.975)
lower <- mle$par - z * se
upper <- mle$par + z * se
results <- data.frame(
Estimate = mle$par,
StdError = se,
CI_Lower = lower,
CI_Upper = upper,
row.names = c("beta_netflix", "beta_prime", "beta_ads", "beta_price")
)
print(round(results, 4))
log_prior <- function(beta) {
sum(dnorm(beta[1:3], mean = 0, sd = sqrt(5), log = TRUE)) +
dnorm(beta[4], mean = 0, sd = 1, log = TRUE)
}
# Combine into log posterior
log_posterior <- function(beta, data) {
log_likelihood(beta, data) + log_prior(beta)
}
log_prior <- function(beta) {
sum(dnorm(beta[1:3], mean = 0, sd = sqrt(5), log = TRUE)) +
dnorm(beta[4], mean = 0, sd = 1, log = TRUE)
}
# Combine into log posterior
log_posterior <- function(beta, data) {
log_likelihood(beta, data) + log_prior(beta)
}
set.seed(123)
n_iter <- 11000
burn_in <- 1000
n_keep <- n_iter - burn_in
n_params <- 4
# Initialize storage
beta_samples <- matrix(NA, nrow = n_iter, ncol = n_params)
colnames(beta_samples) <- c("beta_netflix", "beta_prime", "beta_ads", "beta_price")
# Starting values
beta_current <- c(0, 0, 0, 0)
log_post_current <- log_posterior(beta_current, conjoint_data)
# Proposal SDs (univariate sampling)
proposal_sds <- c(0.05, 0.05, 0.05, 0.005)
for (s in 1:n_iter) {
beta_proposal <- rnorm(n_params, mean = beta_current, sd = proposal_sds)
log_post_proposal <- log_posterior(beta_proposal, conjoint_data)
# MH acceptance ratio
log_accept_ratio <- log_post_proposal - log_post_current
if (log(runif(1)) < log_accept_ratio) {
beta_current <- beta_proposal
log_post_current <- log_post_proposal
}
beta_samples[s, ] <- beta_current
}
posterior_samples <- beta_samples[(burn_in + 1):n_iter, ]
posterior_summary <- apply(posterior_samples, 2, function(x) {
c(mean = mean(x),
sd = sd(x),
CI_lower = quantile(x, 0.025),
CI_upper = quantile(x, 0.975))
})
print(round(t(posterior_summary), 4))
# Extract beta_price samples
beta_price_samples <- posterior_samples[, "beta_price"]
# Set up plotting area: 1 row, 2 columns
par(mfrow = c(1, 2))
# --- Trace plot ---
plot(beta_price_samples, type = "l",
main = "Trace Plot: Beta_Price",
xlab = "Iteration", ylab = "Value")
# --- Histogram of posterior ---
hist(beta_price_samples, breaks = 40, col = "lightblue", border = "white",
main = "Posterior of Beta_Price",
xlab = "Value", probability = TRUE)
# Optional: add posterior mean as a red line
abline(v = mean(beta_price_samples), col = "red", lwd = 2)
# Posterior summaries
posterior_summary <- apply(posterior_samples, 2, function(x) {
c(mean = mean(x),
sd = sd(x),
CI_lower = quantile(x, 0.025),
CI_upper = quantile(x, 0.975))
})
# Format results
posterior_summary <- round(t(posterior_summary), 4)
print(posterior_summary)
